{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from spell_checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    \"\"\"\n",
    "    This data structure is the value of the indices dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, pointer2postingsList):\n",
    "        #size of the postings list\n",
    "        \n",
    "        self.size = size\n",
    "        #pointer to the head of the postings list\n",
    "        self.pointer2postingsList = pointer2postingsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingNode:\n",
    "    \"\"\"\n",
    "    Linked list for the postings list\n",
    "    \"\"\"\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker(object):\n",
    "\n",
    "    DEFAULT_DICTIONARIES = {'english': open('englishdic.sec', 'r').read().splitlines(),\n",
    "            'german': open('germandic-utf8.sec', 'r').read().splitlines()}\n",
    "\n",
    "    def __init__(self, lang_vocab: list, fdist: dict = None, max_edit_distance: int = 2):\n",
    "        self.alphabet = 'aäbcdefghijklmnoöpqrsßtuüvwxyz'\n",
    "        self.lang_vocab = {letter: [word.lower() for word in lang_vocab \\\n",
    "            if word.lower().startswith(letter)] for letter in self.alphabet}\n",
    "        self.fdist = fdist\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        \n",
    "        # If no fdist provided and dict is default English, use the Brown news corpus'.\n",
    "        if self.fdist is None: \n",
    "            if lang_vocab == SpellChecker.DEFAULT_DICTIONARIES['english']:\n",
    "                from nltk.corpus import brown\n",
    "                from nltk import FreqDist\n",
    "\n",
    "                self.fdist = FreqDist(w.lower() for w in brown.words(categories='news'))\n",
    "\n",
    "            else:\n",
    "                raise TypeError('No frequency distribution index provided.')\n",
    "\n",
    "    def word_probability(self, word: str) -> int: \n",
    "        \"\"\"Divides the frequency of a word by overall token count.\"\"\"\n",
    "        try:\n",
    "            return self.fdist[word.lower()] / len(self.fdist.keys())\n",
    "        except KeyError:\n",
    "            return 0.0\n",
    "\n",
    "    def spell_check(self, word: str) -> str:\n",
    "        # return max(self.candidates(word), key=self.word_probability)\n",
    "        cans = self.candidates(word)\n",
    "        if cans: return list(cans)[0]\n",
    "\n",
    "    def candidates(self, word: str) -> tuple:\n",
    "        return (self.known([word.lower()]) or                       # word if it is known\n",
    "                self.known(self.edit_distance1(word.lower())) or    # known words with edit distance 1\n",
    "                self.known(self.edit_distance2(word.lower())) or    # known words with edit distance 2\n",
    "                [word])                                     # word, unknown\n",
    "\n",
    "    def known(self, words: list) -> set:\n",
    "        return set(w for w in words if len(w) > 1 \\\n",
    "            and w.lower() in self.lang_vocab[w[0].lower()])\n",
    "\n",
    "    def edit_distance1(self, word: str) -> set:\n",
    "        \"\"\"\n",
    "        Thanks to Peter Norvig (http://norvig.com/spell-correct.html)\n",
    "\n",
    "        Creates all the possible letter combinations that can be made\n",
    "        with an edit distance of 1 to the word.\n",
    "\n",
    "        splits = all ways of dividing the word, e.g. \n",
    "            'word' -> ('w', 'ord'); useful for making changes\n",
    "        deletions = all ways of removing a single letter, e.g.\n",
    "            'word'-> 'ord'\n",
    "        transpositions = all ways of swapping two letters immediately\n",
    "            adjacent to one another, e.g. 'word' -> 'owrd'\n",
    "        replacements = all ways of replacing a letter with another\n",
    "            letter, e.g. 'word' -> 'zord'\n",
    "        insertions = all ways of inserting a letter at any point in the\n",
    "            word, e.g. 'word' -> 'wgord'\n",
    "\n",
    "        :param str word: the relevant word\n",
    "        :return: a set of terms with an edit distance of 1 from the word\n",
    "        :rtype: set\n",
    "        \"\"\"\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletions = [left + right[1:] for left, right in splits if right]\n",
    "        transpositions = [left + right[1] + right[0] + right[2:]\n",
    "                for left, right in splits if len(right) > 1]\n",
    "        replacements = [left + letter + right[1:] for left, right\n",
    "                in splits if right for letter in self.alphabet]\n",
    "        insertions = [left + letter + right for left, right in splits\n",
    "                for letter in self.alphabet]\n",
    "\n",
    "        return set(deletions + transpositions + replacements + insertions)\n",
    "\n",
    "    def edit_distance2(self, word: str) -> set:\n",
    "        #TODO: Should be obsolete now\n",
    "        \"\"\"Simply runs edit_distance1 on every result from edit_distance1(word)\"\"\"\n",
    "        return set(edit2 for edit in self.edit_distance1(word) for edit2\n",
    "            in self.edit_distance1(edit))\n",
    "        \n",
    "    def edit_distanceN(self, word: str) -> set:\n",
    "        #FIXME\n",
    "        \"\"\"Runs `edit_distance1` on the results of `edit_distance1` n times.\"\"\"\n",
    "        ret_val = set(word)\n",
    "\n",
    "        for _ in range(self.max_edit_distance):\n",
    "            for val in ret_val:\n",
    "                ret_val = ret_val | self.edit_distance1(val)\n",
    "\n",
    "        return list(ret_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterIR(object):\n",
    "    \"\"\"\n",
    "    Main Class for the information retrieval task.\n",
    "    \"\"\"\n",
    "    __slots__ = 'id2doc', 'tokenizer', 'unicodes2remove', 'indices', \\\n",
    "    'urlregex', 'punctuation', 'emojis', 'stop_words', 'englishdic', 'germandic', \\\n",
    "    'engSpellCheck', 'gerSpellCheck'\n",
    "\n",
    "    def __init__(self):\n",
    "        #the original mapping from the id's to the tweets, \n",
    "        #which is kept until the end to index the tweets\n",
    "        self.id2doc = {}\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        #bunch of punctuation unicodes which are not in 'string.punctuation'\n",
    "        self.unicodes2remove = [\n",
    "            #all kinds of quotes\n",
    "            u'\\u2018', u'\\u2019', u'\\u201a', u'\\u201b', u'\\u201c',\\\n",
    "            u'\\u201d', u'\\u201e', u'\\u201f', u'\\u2014',\n",
    "            #all kinds of hyphens\n",
    "            u'\\u002d', u'\\u058a', u'\\u05be', u'\\u1400', u'\\u1806',\\\n",
    "            u'\\u2010', u'\\u2011', u'\\u2012', u'\\u2013',\n",
    "            u'\\u2014', u'\\u2015', u'\\u2e17', u'\\u2e1a', u'\\u2e3a',\\\n",
    "            u'\\u2e3b', u'\\u2e40', u'\\u301c', u'\\u3030',\n",
    "            u'\\u30a0', u'\\ufe31', u'\\ufe32', u'\\ufe58', u'\\ufe63',\\\n",
    "            u'\\uff0d', u'\\u00b4'\n",
    "        ]\n",
    "        #the resulting datastructure which has the tokens as keys\n",
    "        #and the Index objects as values\n",
    "        self.indices = {}\n",
    "        #regex to match urls (taken from the web)\n",
    "        self.urlregex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]'\n",
    "                                   '|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        #keep @ to be able to recognize usernames\n",
    "        self.punctuation = string.punctuation.replace('@', '') + \\\n",
    "        ''.join(self.unicodes2remove)\n",
    "        self.punctuation = self.punctuation.replace('#', '')\n",
    "        #a bunch of emoji unicodes\n",
    "        self.emojis = ''.join(emoji.UNICODE_EMOJI)\n",
    "        self.emojis = self.emojis.replace('#', '')\n",
    "        #combined english and german stop words\n",
    "        self.stop_words = set(stopwords.words('english') + stopwords.words('german'))\n",
    "        self.englishdic = set()\n",
    "        self.germandic = set()\n",
    "        self.engSpellCheck = None\n",
    "        self.gerSpellCheck = None\n",
    "        \n",
    "\n",
    "    def initId2doc(self, path):\n",
    "        \"\"\"\n",
    "        Reads the file in and fills the id2doc datastructure.\n",
    "        :param path: path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "            r = csv.reader(f, delimiter='\\t')\n",
    "            for line in r:\n",
    "                self.id2doc[line[1]] = line[4]\n",
    "        f.close()\n",
    "    \n",
    "    def initLanguageDics(self, path_dir, filenameEN='englishdic.sec', filenameGER='germandic.sec'):\n",
    "        if filenameEN:\n",
    "            with open(path_dir+filenameEN, 'r', encoding='utf-8') as f:\n",
    "                for word in f.readlines():\n",
    "                    self.englishdic.add(word.lower().strip())\n",
    "                f.close()\n",
    "            \n",
    "        if filenameGER:\n",
    "            with open(path_dir+filenameGER, 'r', encoding='utf-8') as f:\n",
    "                for word in f.readlines():\n",
    "                    self.germandic.add(word.lower().strip())\n",
    "                f.close()\n",
    "                \n",
    "    def _initSpellCheck(self, dic_path):\n",
    "        return SpellChecker(open(dic_path).read().splitlines(),\n",
    "                fdist={term: node.size for (term, node) in self.indices.items()}).spell_check\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices.keys())\n",
    "                \n",
    "\n",
    "    def clean(self, s):\n",
    "        \"\"\"\n",
    "        Normalizes a string (tweet) by removing the urls, punctuation, digits,\n",
    "        emojis, by putting everything to lowercase and removing the\n",
    "        stop words. Tokenization is performed aswell.\n",
    "        \n",
    "        :param s the string (tweet) to clean\n",
    "        :return: returns a list of cleaned tokens\n",
    "        \"\"\"\n",
    "        s = ' '.join(s.replace('[NEWLINE]', '').split())\n",
    "        s = self.urlregex.sub('', s).strip()\n",
    "        s = s.translate(str.maketrans('', '', self.punctuation + string.digits \\\n",
    "                                      + self.emojis)).strip()\n",
    "        s = ' '.join(s.split())\n",
    "        s = s.lower()\n",
    "        s = self.tokenizer.tokenize(s)\n",
    "        s = [w for w in s if w not in self.stop_words]\n",
    "        return s\n",
    "\n",
    "    def index(self, path):\n",
    "        \"\"\"\n",
    "        1) call the method to read the file in\n",
    "        2) iterate over the original datastructure id2doc which keeps the mapping\n",
    "        of the tweet ids to the actual tweets and do:\n",
    "            2a) preprocessing of the tweets\n",
    "            2b) create a mapping from each token to its postings list (tokens2id)\n",
    "        3) iterate over the just created mapping of tokens to their respective \n",
    "        postings lists (tokens2id) and do:\n",
    "            3a) calculate the size of the postingslist\n",
    "            3b) sort the postings list numerically in ascending order\n",
    "            3c) create a linked list for the postings list\n",
    "            3d) create the Index object with the size of the postings list and\n",
    "            the pointer to the postings list - add to the resulting datastructure \n",
    "        :param path: the path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.initLanguageDics('./', filenameGER='germandic-utf8.sec')\n",
    "        self.initId2doc(path)\n",
    "        self.engSpellCheck = self._initSpellCheck('englishdic.sec')\n",
    "        self.gerSpellCheck = self._initSpellCheck('germandic-utf8.sec')\n",
    "        tokens2id = {}\n",
    "        for id,doc in self.id2doc.items():\n",
    "            doc = self.clean(doc)\n",
    "            language = self._detectLanguage(' '.join(doc))\n",
    "            print(doc)\n",
    "            print(language)\n",
    "\n",
    "            for t in doc:\n",
    "                print(\"Word to check: \",t)\n",
    "                if language == 'english':\n",
    "                    if t[0] not in ['@', '#'] and t not in self.englishdic:\n",
    "                        print(\"EN Corrected from: \", t)\n",
    "                        t = self.spellCheck(t, language)\n",
    "                        print(\"To: \", t)\n",
    "                elif language == 'german':\n",
    "                    if t[0] not in ['@', '#'] and t not in self.germandic:\n",
    "                        print(\"DE Corrected from: \", t)\n",
    "                        t = self.spellCheck(t, language)\n",
    "                        print(\"To: \", t)\n",
    "                        \n",
    "                if t in tokens2id.keys():\n",
    "                    tokens2id[t].add(id)\n",
    "                else:\n",
    "                    #a set is used to avoid multiple entries of the same tweetID\n",
    "                    tokens2id[t] = {id}\n",
    "\n",
    "        for t,ids in tokens2id.items():\n",
    "            #size of the postings list which belongs to token t\n",
    "            size = len(ids)\n",
    "            #sort in ascending order\n",
    "            ids = sorted(ids)\n",
    "            #use the first (and smallest) tweetID to be the head node of the \n",
    "            #linked list\n",
    "            node = PostingNode(ids[0])\n",
    "            #keep reference to the head of the linked list since node variable\n",
    "            #is going to be overridden\n",
    "            pointer = node\n",
    "            for id in ids[1:]:\n",
    "                #create further list items\n",
    "                n = PostingNode(id)\n",
    "                #and append to the linked list\n",
    "                node.next = n\n",
    "                #step further\n",
    "                node = n\n",
    "            #create the index object with size of the postings list \n",
    "            #and a link to the postings list itself\n",
    "            i = Index(size, pointer)\n",
    "            self.indices[t] = i\n",
    "            \n",
    "        \n",
    "    def _detectLanguage(self, context):\n",
    "        tokens = self.tokenizer.tokenize(context)\n",
    "        stopsEN = [token for token in tokens if token in stopwords.words('english')]\n",
    "        stopsDE = [token for token in tokens if token in stopwords.words('german')]\n",
    "        if len(stopsEN) > len(stopsDE):\n",
    "            return 'english'\n",
    "        elif len(stopsDE) > len(stopsEN):\n",
    "            return 'german'\n",
    "        else:\n",
    "            cleaned = self.clean(context)\n",
    "\n",
    "            wordsEN = []\n",
    "            wordsDE = []\n",
    "            for token in cleaned:\n",
    "                if token in self.englishdic:\n",
    "                    wordsEN.append(token)\n",
    "                if token in self.germandic:\n",
    "                    wordsDE.append(token)\n",
    "            if len(wordsEN) > len(wordsDE):\n",
    "                return 'english'\n",
    "            elif len(wordsDE) > len(wordsEN):\n",
    "                return 'german'\n",
    "            else:\n",
    "                return 'english'\n",
    "\n",
    "    def _query(self, term, lang):\n",
    "        \"\"\"\n",
    "        Internal method to query for one term.\n",
    "        :param: term the word which was queried for \n",
    "        :return: returns the Index object of the corresponding query term\n",
    "        \"\"\"\n",
    "        print(\"old: \",term)\n",
    "        if lang == 'english':\n",
    "            if term not in self.englishdic:\n",
    "                term = self.spellCheck(term, lang)\n",
    "                print(\"new: \", term)\n",
    "                print()\n",
    "        elif lang == 'german':\n",
    "            if term not in self.germandic:\n",
    "                term = self.spellCheck(term, lang)\n",
    "                print(\"new: \", term)\n",
    "                print()\n",
    "                \n",
    "        try:\n",
    "            return self.indices[term]\n",
    "        except KeyError:\n",
    "            return Index(0, PostingNode(''))\n",
    "    \n",
    "    def spellCheck(self, term, lang):\n",
    "        return {'english': self.engSpellCheck,\n",
    "                'german': self.gerSpellCheck}[lang](term)\n",
    "\n",
    "    def query(self, *arg):\n",
    "        \"\"\"\n",
    "        Query method which can take any number of terms as arguments.\n",
    "        It uses the internal _query method to get the postings lists for the single \n",
    "        terms. It calculates the intersection of all postings lists.\n",
    "        :param *arg term arguments\n",
    "        :return: returns a list of tweetIDs which all contain the query terms\n",
    "        \"\"\"\n",
    "        language = self._detectLanguage(' '.join([t for t in arg]))\n",
    "        print(language)\n",
    "        #[self._query(t, language) for t in arg if t not in self.stop_words]\n",
    "        #sys.exit(0)\n",
    "        \n",
    "        #at this point it's a list of Index objects\n",
    "        pointers = [self._query(t, language) for t in arg if t not in self.stop_words]\n",
    "        #here the Index objects get sorted by the size of the \n",
    "        #postings list they point to\n",
    "        pointers = sorted(pointers, key=lambda i: i.size)\n",
    "        #here it becomes a list of pointers to the postings lists\n",
    "        pointers = [i.pointer2postingsList for i in pointers]\n",
    "        #first pointer\n",
    "        intersection = pointers[0]\n",
    "        #step through the pointers\n",
    "        for p in pointers[1:]:\n",
    "            #intersection between the new postings list and the so far\n",
    "            #computed intersection\n",
    "            intersection = self.intersect(intersection, p)\n",
    "            #if at any point the intersection is empty there is \n",
    "            #no need to continue\n",
    "            if not intersection:\n",
    "                return []\n",
    "        #convert the resulting intersection to a normal list\n",
    "        rval = []\n",
    "        pointer = intersection\n",
    "        while pointer:\n",
    "            rval.append(pointer.val)\n",
    "            pointer = pointer.next\n",
    "            \n",
    "        return rval\n",
    "    \n",
    "    def intersect(self, pointer1, pointer2):\n",
    "        \"\"\"\n",
    "        Computes the intersection for two postings lists.\n",
    "        :param pointer1: first postings list\n",
    "        :param pointer2: second postings list\n",
    "        :return: returns the intersection \n",
    "        \"\"\"\n",
    "        #create temporary head node\n",
    "        node = PostingNode('tmp')\n",
    "        #keep reference to head node\n",
    "        rvalpointer = node\n",
    "        while pointer1 and pointer2:\n",
    "            val1 = pointer1.val\n",
    "            val2 = pointer2.val\n",
    "            #only append to the linked list if the values are equal\n",
    "            if val1 == val2:\n",
    "                n = PostingNode(val1)\n",
    "                node.next = n\n",
    "                node = n\n",
    "                pointer1 = pointer1.next\n",
    "                pointer2 = pointer2.next\n",
    "            #otherwise the postings list with the smaller value \n",
    "            #at the current index moves one forward\n",
    "            elif val1 > val2:\n",
    "                pointer2 = pointer2.next\n",
    "            elif val1 < val2:\n",
    "                pointer1 = pointer1.next\n",
    "        #return from the second element on since the first was the temporary one\n",
    "        return rvalpointer.next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@knakatani', '@chikonjugular', '@joofford', '@steveblogs', 'says', 'lifetime', 'risk', 'cervical', 'cancer', 'japan', 'means', 'hpv', 'endemic', 'japan', 'screening', 'working', 'well']\nenglish\nWord to check:  @knakatani\nWord to check:  @chikonjugular\nWord to check:  @joofford\nWord to check:  @steveblogs\nWord to check:  says\nWord to check:  lifetime\nWord to check:  risk\nWord to check:  cervical\nWord to check:  cancer\nWord to check:  japan\nWord to check:  means\nWord to check:  hpv\nEN Corrected from:  hpv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  hew\nWord to check:  endemic\nWord to check:  japan\nWord to check:  screening\nWord to check:  working\nWord to check:  well\n['@fischerkurt', 'lady', 'whats', 'tumor', '#kippcharts']\nenglish\nWord to check:  @fischerkurt\nWord to check:  lady\nWord to check:  whats\nWord to check:  tumor\nWord to check:  #kippcharts\n['@kingsofmetal', 'diagnoseverdacht', 'nunmal', 'schwer', 'gerade', 'hausarzt', 'blutbild', 'meist', 'sehen', 'gerade', 'hormone', 'überprüft', 'erklärbare', 'gewichtseinlagerungen', 'ja', 'wasser', 'fett', 'kind', 'tumor']\ngerman\nWord to check:  @kingsofmetal\nWord to check:  diagnoseverdacht\nWord to check:  nunmal\nDE Corrected from:  nunmal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  neunmal\nWord to check:  schwer\nWord to check:  gerade\nWord to check:  hausarzt\nWord to check:  blutbild\nWord to check:  meist\nWord to check:  sehen\nWord to check:  gerade\nWord to check:  hormone\nWord to check:  überprüft\nWord to check:  erklärbare\nWord to check:  gewichtseinlagerungen\nWord to check:  ja\nWord to check:  wasser\nWord to check:  fett\nWord to check:  kind\nWord to check:  tumor\n['@germanletsplay', '@quentin', '@lopoopl', '@levanni', '@igeloe', '@annelle', 'glückwunsch']\ngerman\nWord to check:  @germanletsplay\nWord to check:  @quentin\nWord to check:  @lopoopl\nWord to check:  @levanni\nWord to check:  @igeloe\nWord to check:  @annelle\nWord to check:  glückwunsch\n['interesting', 'pcr', 'rate', 'major', 'centers', 'authors', 'argue', 'treatment', 'compliance', 'major', 'centers', 'see', 'database', 'think', 'rather', 'due', 'earlier', 'detection', 'smaller', 'tumors', 'pcr', 'look', 'deeper', '#crcsm']\nenglish\nWord to check:  interesting\nWord to check:  pcr\nEN Corrected from:  pcr\nTo:  pyr\nWord to check:  rate\nWord to check:  major\nWord to check:  centers\nWord to check:  authors\nWord to check:  argue\nWord to check:  treatment\nWord to check:  compliance\nWord to check:  major\nWord to check:  centers\nWord to check:  see\nWord to check:  database\nWord to check:  think\nWord to check:  rather\nWord to check:  due\nWord to check:  earlier\nWord to check:  detection\nWord to check:  smaller\nWord to check:  tumors\nWord to check:  pcr\nEN Corrected from:  pcr\nTo:  pyr\nWord to check:  look\nWord to check:  deeper\nWord to check:  #crcsm\n['new', 'nanobots', 'kill', 'cancerous', 'tumors', 'cutting', 'blood', 'supply', '#digitaleconomy', 'february', 'pm']\nenglish\nWord to check:  new\nWord to check:  nanobots\nWord to check:  kill\nWord to check:  cancerous\nWord to check:  tumors\nWord to check:  cutting\nWord to check:  blood\nWord to check:  supply\nWord to check:  #digitaleconomy\nWord to check:  february\nWord to check:  pm\nEN Corrected from:  pm\nTo:  pi\n['rip', 'salem', 'aufgrund', 'tumors', 'eingeschläfert']\ngerman\nWord to check:  rip\nDE Corrected from:  rip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  rap\nWord to check:  salem\nWord to check:  aufgrund\nWord to check:  tumors\nWord to check:  eingeschläfert\n['cancerfighting', 'nanorobots', 'programmed', 'seek', 'destroy', 'tumors', 'study', 'shows', 'first', 'applications', 'dna', 'origami', 'nanomedicine', 'nanoroboter', 'schrumpfen', 'tumore', '#medtech']\nenglish\nWord to check:  cancerfighting\nEN Corrected from:  cancerfighting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  cancerfighting\nWord to check:  nanorobots\nEN Corrected from:  nanorobots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  nanobots\nWord to check:  programmed\nWord to check:  seek\nWord to check:  destroy\nWord to check:  tumors\nWord to check:  study\nWord to check:  shows\nWord to check:  first\nWord to check:  applications\nEN Corrected from:  applications\nTo:  application\nWord to check:  dna\nWord to check:  origami\nWord to check:  nanomedicine\nEN Corrected from:  nanomedicine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  nanomedicine\nWord to check:  nanoroboter\nEN Corrected from:  nanoroboter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  nanoroboter\nWord to check:  schrumpfen\nEN Corrected from:  schrumpfen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  schrumpfen\nWord to check:  tumore\nEN Corrected from:  tumore\nTo:  tumored\nWord to check:  #medtech\n['@riptear', 'tumorsdat', 'leg', 'straight', 'mccain']\ngerman\nWord to check:  @riptear\nWord to check:  tumorsdat\nDE Corrected from:  tumorsdat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  tumorsdat\nWord to check:  leg\nWord to check:  straight\nWord to check:  mccain\n['quote', 'one', 'statement', 'cancers']\nenglish\nWord to check:  quote\nWord to check:  one\nWord to check:  statement\nWord to check:  cancers\nEN Corrected from:  cancers\nTo:  cancer\n['@joyannreid', '@pampylu', 'wrong', 'think', 'trump', 'probleme', 'mere', 'small', 'symptom', 'systematic', 'cancer', 'ask', 'antigunkids', 'go', 'look', 'failures', 'us', 'democracy']\nenglish\nWord to check:  @joyannreid\nWord to check:  @pampylu\nWord to check:  wrong\nWord to check:  think\nWord to check:  trump\nWord to check:  probleme\nEN Corrected from:  probleme\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  problem\nWord to check:  mere\nWord to check:  small\nWord to check:  symptom\nWord to check:  systematic\nWord to check:  cancer\nWord to check:  ask\nWord to check:  antigunkids\nEN Corrected from:  antigunkids\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  antigunkids\nWord to check:  go\nWord to check:  look\nWord to check:  failures\nEN Corrected from:  failures\nTo:  failure\nWord to check:  us\nWord to check:  democracy\n['erstmal', 'nen', 'anti', 'cancer', 'stick', 'lunge', 'reintherapieren']\nenglish\nWord to check:  erstmal\nEN Corrected from:  erstmal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  resteal\nWord to check:  nen\nEN Corrected from:  nen\nTo:  nun\nWord to check:  anti\nWord to check:  cancer\nWord to check:  stick\nWord to check:  lunge\nWord to check:  reintherapieren\nEN Corrected from:  reintherapieren\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  reintherapieren\n['#usa', '#upi', '#news', 'broadcast', '#emetnewspress', 'obesity', 'may', 'cause', 'sudden', 'cardiac', 'arrest', 'young', 'people', 'study', 'says', 'obesity', 'high', 'blood', 'pressure', 'may', 'play', 'much', 'greater', 'role', 'sudden', 'cardiac', 'arrest', 'among', 'young', 'people', 'previously', 'thought', 'ne']\nenglish\nWord to check:  #usa\nWord to check:  #upi\nWord to check:  #news\nWord to check:  broadcast\nWord to check:  #emetnewspress\nWord to check:  obesity\nWord to check:  may\nWord to check:  cause\nWord to check:  sudden\nWord to check:  cardiac\nWord to check:  arrest\nWord to check:  young\nWord to check:  people\nWord to check:  study\nWord to check:  says\nWord to check:  obesity"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nWord to check:  high\nWord to check:  blood\nWord to check:  pressure\nWord to check:  may\nWord to check:  play\nWord to check:  much\nWord to check:  greater\nWord to check:  role\nWord to check:  sudden\nWord to check:  cardiac\nWord to check:  arrest\nWord to check:  among\nWord to check:  young\nWord to check:  people\nWord to check:  previously\nWord to check:  thought\nWord to check:  ne\n['leseempfehlung', 'extraordinary', 'correlation', 'obesity', 'social', 'inequality']\nenglish\nWord to check:  leseempfehlung\nEN Corrected from:  leseempfehlung\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  leseempfehlung\nWord to check:  extraordinary\nWord to check:  correlation\nWord to check:  obesity\nWord to check:  social\nWord to check:  inequality\n['@fazefuzzface', 'welcome', 'obesity']\nenglish\nWord to check:  @fazefuzzface\nWord to check:  welcome\nWord to check:  obesity\n['@isonnylucas', 'thats', 'exactly', 'point', 'whataboutism', 'dont', 'want', 'face', 'problem', 'point', 'worse', 'problem', 'bfollowing', 'logic', 'yes', 'opioid', 'crisis', 'bad', 'obesity', 'affects', 'way', 'children', 'many', 'deathslike', 'never', 'solve', 'problem']\nenglish\nWord to check:  @isonnylucas\nWord to check:  thats\nWord to check:  exactly\nWord to check:  point\nWord to check:  whataboutism\nEN Corrected from:  whataboutism\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  whatabouts\nWord to check:  dont\nWord to check:  want\nWord to check:  face\nWord to check:  problem\nWord to check:  point\nWord to check:  worse\nWord to check:  problem\nWord to check:  bfollowing\nEN Corrected from:  bfollowing\nTo:  following\nWord to check:  logic\nWord to check:  yes\nWord to check:  opioid\nEN Corrected from:  opioid\nTo:  apioid\nWord to check:  crisis\nWord to check:  bad\nWord to check:  obesity\nWord to check:  affects\nEN Corrected from:  affects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To:  affect\nWord to check:  way\nWord to check:  children\nEN Corrected from:  children\n"
     ]
    }
   ],
   "source": [
    "twitterIR = TwitterIR()\n",
    "twitterIR.index('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
