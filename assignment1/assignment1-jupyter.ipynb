{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from spell_checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    \"\"\"\n",
    "    This data structure is the value of the indices dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, pointer2postingsList):\n",
    "        # size of the postings list\n",
    "        \n",
    "        self.size = size\n",
    "        # pointer to the head of the postings list\n",
    "        self.pointer2postingsList = pointer2postingsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingNode:\n",
    "    \"\"\"\n",
    "    Linked list for the postings list\n",
    "    \"\"\"\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker(object):\n",
    "\tDEFAULT_DICTIONARIES = {'english': open('englishdic.sec', 'r').read().splitlines(),\n",
    "\t                        'german': open('germandic-utf8.sec', 'r').read().splitlines()}\n",
    "\n",
    "\tdef __init__(self, lang_vocab: list, fdist: dict = None, max_edit_distance: int = 2):\n",
    "\t\t\"\"\"\n",
    "\t\tCreates a `SpellChecker` object from a dictionary and a frequency distribution. The principle\n",
    "\t\tmethod is `spell_check`. Every other method is called in calling it. To best understand this\n",
    "\t\tclass, start there and work your way through the call sequence: `spell_check` calls\n",
    "\t\t`candidates` which in turns calls `known` and the edit distance methods.\n",
    "\t\t\n",
    "\t\t:param lang_vocab: a list of words in a language's vocabulary\n",
    "\t\t:param fdist: a dictionary-like frequency distribution from a large corpus;\n",
    "\t\t\t\tif none is provided and the class default English dictionary is supplied to\n",
    "\t\t\t\t`lang_vocab`, the Brown corpus is imported and used\n",
    "\t\t:param max_edit_distance: the maximum edit distance at which words will still be considered\n",
    "\t\t\"\"\"\n",
    "\t\t# At present, sticking with the German alphabet even for English\n",
    "\t\tself.alphabet = 'aäbcdefghijklmnoöpqrsßtuüvwxyz'\n",
    "\t\t# Key:Value pair of alphabet letters and lists of words beginning with those letters\n",
    "\t\t# in the provided list of dictionary terms to decrease the time it takes to do dictionary lookups.\n",
    "\t\tself.lang_vocab = {letter: [word.lower() for word in lang_vocab \\\n",
    "\t\t                            if word.lower().startswith(letter)] for letter in self.alphabet}\n",
    "\t\tself.fdist = fdist\n",
    "\t\tself.max_edit_distance = max_edit_distance\n",
    "\n",
    "\t\t# If no fdist provided and `lang_vocab` is default English, use the Brown news corpus'.\n",
    "\t\t# In case you don't have a corpus big enough to create a strong frequency distribution\n",
    "\t\tif self.fdist is None:\n",
    "\t\t\tif lang_vocab == SpellChecker.DEFAULT_DICTIONARIES['english']:\n",
    "\t\t\t\tfrom nltk.corpus import brown\n",
    "\t\t\t\tfrom nltk import FreqDist\n",
    "\n",
    "\t\t\t\tself.fdist = FreqDist(w.lower() for w in brown.words(categories='news'))\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\traise TypeError('No frequency distribution index provided.')\n",
    "\n",
    "\tdef candidates(self, word: str) -> set:\n",
    "\t\t\"\"\"\n",
    "\t\tReturns words within an edit distance of 2 in a ranked order, only generating words\n",
    "\t\tif there are no results from the previous method. If the word does not begin with\n",
    "\t\ta letter in `self.alphabet`, it is returned immediately as it was given.\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\treturn (self.known([word.lower()]) or                     # word if it is known\n",
    "\t\t\t        self.known(self.edit_distance1(word.lower())) or  # known words with edit distance 1\n",
    "\t\t\t        self.known(self.edit_distance2(word.lower())) or  # known words with edit distance 2\n",
    "\t\t\t        [word])                                           # word, unknown\n",
    "\t\texcept KeyError:\n",
    "\t\t\treturn [word]\n",
    "\t\t\t\n",
    "\tdef edit_distance1(self, word: str) -> set:\n",
    "\t\t\"\"\"\n",
    "        Thanks to Peter Norvig (http://norvig.com/spell-correct.html)\n",
    "\n",
    "        Creates all the possible letter combinations that can be made\n",
    "        with an edit distance of 1 to the word.\n",
    "\n",
    "        splits = all ways of dividing the word, e.g. \n",
    "            'word' -> ('w', 'ord'); useful for making changes\n",
    "        deletions = all ways of removing a single letter, e.g.\n",
    "            'word'-> 'ord'\n",
    "        transpositions = all ways of swapping two letters immediately\n",
    "            adjacent to one another, e.g. 'word' -> 'owrd'\n",
    "        replacements = all ways of replacing a letter with another\n",
    "            letter, e.g. 'word' -> 'zord'\n",
    "        insertions = all ways of inserting a letter at any point in the\n",
    "            word, e.g. 'word' -> 'wgord'\n",
    "\n",
    "        :param str word: the relevant word\n",
    "        :return: a set of terms with an edit distance of 1 from the word\n",
    "        :rtype: set\n",
    "        \"\"\"\n",
    "\t\tsplits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\t\tdeletions = [left + right[1:] for left, right in splits if right]\n",
    "\t\ttranspositions = [left + right[1] + right[0] + right[2:]\n",
    "\t\t                  for left, right in splits if len(right) > 1]\n",
    "\t\treplacements = [left + letter + right[1:] for left, right\n",
    "\t\t                in splits if right for letter in self.alphabet]\n",
    "\t\tinsertions = [left + letter + right for left, right in splits\n",
    "\t\t              for letter in self.alphabet]\n",
    "\n",
    "\t\treturn set(deletions + transpositions + replacements + insertions)\n",
    "\n",
    "\tdef edit_distance2(self, word: str) -> set:\n",
    "\t\t\"\"\"Simply runs edit_distance1 on every result from edit_distance1(word)\"\"\"\n",
    "\t\treturn set(edit2 for edit in self.edit_distance1(word) for edit2\n",
    "\t\t           in self.edit_distance1(edit))\n",
    "\n",
    "\tdef edit_distanceN(self, word: str) -> set:\n",
    "\t\t# FIXME\n",
    "\t\t\"\"\"Runs `edit_distance1` on the results of `edit_distance1` n times.\"\"\"\n",
    "\t\tret_val = set(word)\n",
    "\n",
    "\t\tfor _ in range(self.max_edit_distance):\n",
    "\t\t\tfor val in ret_val:\n",
    "\t\t\t\tret_val = ret_val | self.edit_distance1(val)\n",
    "\n",
    "\t\treturn ret_val\n",
    "\t\n",
    "\tdef known(self, words: list) -> set:\n",
    "\t\t\"\"\"\n",
    "\t\tWalks through words in a list, checks them against `lang_vocab`,\n",
    "\t\tand returns a set of those that match.\n",
    "\t\t\"\"\"\n",
    "\t\treturn set(w for w in words if len(w) > 1\n",
    "\t\t           and w.lower() in self.lang_vocab[w[0].lower()])\n",
    "\n",
    "\tdef spell_check(self, word: str) -> str:\n",
    "\t\t\"\"\"Chooses the most likely word in a set of candidates based on `word_probability`.\"\"\"\n",
    "\t\treturn max(self.candidates(word), key=self.word_probability)\n",
    "\t\t\n",
    "\tdef word_probability(self, word: str) -> int:\n",
    "\t\t\"\"\"Divides the frequency of a word by overall token count.\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\treturn self.fdist[word.lower()] / len(self.fdist.keys())\n",
    "\t\texcept KeyError:\n",
    "\t\t\treturn 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterIR(object):\n",
    "\t\"\"\"\n",
    "    Main Class for the information retrieval task.\n",
    "    \"\"\"\n",
    "\t__slots__ = 'id2doc', 'tokenizer', 'unicodes2remove', 'indices', \\\n",
    "\t            'urlregex', 'punctuation', 'emojis', 'stop_words', 'englishdic', 'germandic', \\\n",
    "\t            'engSpellCheck', 'gerSpellCheck', 'correctedTerms'\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t# the original mapping from the id's to the tweets, \n",
    "\t\t# which is kept until the end to index the tweets\n",
    "\t\tself.id2doc = {}\n",
    "\t\tself.tokenizer = TweetTokenizer()\n",
    "\t\t# bunch of punctuation unicodes which are not in 'string.punctuation'\n",
    "\t\tself.unicodes2remove = [\n",
    "\t\t\t# all kinds of quotes\n",
    "\t\t\tu'\\u2018', u'\\u2019', u'\\u201a', u'\\u201b', u'\\u201c', \\\n",
    "\t\t\tu'\\u201d', u'\\u201e', u'\\u201f', u'\\u2014',\n",
    "\t\t\t# all kinds of hyphens\n",
    "\t\t\tu'\\u002d', u'\\u058a', u'\\u05be', u'\\u1400', u'\\u1806', \\\n",
    "\t\t\tu'\\u2010', u'\\u2011', u'\\u2012', u'\\u2013',\n",
    "\t\t\tu'\\u2014', u'\\u2015', u'\\u2e17', u'\\u2e1a', u'\\u2e3a', \\\n",
    "\t\t\tu'\\u2e3b', u'\\u2e40', u'\\u301c', u'\\u3030',\n",
    "\t\t\tu'\\u30a0', u'\\ufe31', u'\\ufe32', u'\\ufe58', u'\\ufe63', \\\n",
    "\t\t\tu'\\uff0d', u'\\u00b4'\n",
    "\t\t]\n",
    "\t\t# the resulting data structure which has the tokens as keys\n",
    "\t\t# and the Index objects as values\n",
    "\t\tself.indices = {}\n",
    "\t\t# regex to match urls (taken from the web)\n",
    "\t\tself.urlregex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]'\n",
    "\t\t                           '|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\t\t# keep @ to be able to recognize usernames\n",
    "\t\tself.punctuation = string.punctuation.replace('@', '') + \\\n",
    "\t\t                   ''.join(self.unicodes2remove)\n",
    "\t\tself.punctuation = self.punctuation.replace('#', '')\n",
    "\t\tself.punctuation = self.punctuation.replace('…', '')\n",
    "\t\t# a bunch of emoji unicodes\n",
    "\t\tself.emojis = ''.join(emoji.UNICODE_EMOJI)\n",
    "\t\tself.emojis = self.emojis.replace('#', '')\n",
    "\t\t# combined english and german stop words\n",
    "\t\tself.stop_words = set(stopwords.words('english') + stopwords.words('german'))\n",
    "\t\tself.englishdic = set()\n",
    "\t\tself.germandic = set()\n",
    "\t\tself.engSpellCheck = self._initSpellCheck('english')\n",
    "\t\tself.gerSpellCheck = self._initSpellCheck('german')\n",
    "\t\tself.correctedTerms = []    # For demonstration purposes only\n",
    "\n",
    "\tdef clean(self, s):\n",
    "\t\t\"\"\"\n",
    "        Normalizes a string (tweet) by removing the urls, punctuation, digits,\n",
    "        emojis, by putting everything to lowercase and removing the\n",
    "        stop words. Tokenization is performed aswell.\n",
    "        \n",
    "        :param s the string (tweet) to clean\n",
    "        :return: returns a list of cleaned tokens\n",
    "        \"\"\"\n",
    "\t\ts = ' '.join(s.replace('[NEWLINE]', '').split())\n",
    "\t\ts = ' '.join(s.replace('…', '...').split())\n",
    "\t\ts = self.urlregex.sub('', s).strip()\n",
    "\t\ts = s.translate(str.maketrans('', '', self.punctuation + string.digits \\\n",
    "\t\t                              + self.emojis)).strip()\n",
    "\t\ts = ' '.join(s.split())\n",
    "\t\ts = s.lower()\n",
    "\t\ts = self.tokenizer.tokenize(s)\n",
    "\t\ts = [w for w in s if w not in self.stop_words]\n",
    "\t\treturn s\n",
    "\n",
    "\tdef _detectLanguage(self, context):\n",
    "\t\ttokens = self.tokenizer.tokenize(context)\n",
    "\t\tstopsEN = [token for token in tokens if token in stopwords.words('english')]\n",
    "\t\tstopsDE = [token for token in tokens if token in stopwords.words('german')]\n",
    "\t\tif len(stopsEN) > len(stopsDE):\n",
    "\t\t\treturn 'english'\n",
    "\t\telif len(stopsDE) > len(stopsEN):\n",
    "\t\t\treturn 'german'\n",
    "\t\telse:\n",
    "\t\t\tcleaned = self.clean(context)\n",
    "\n",
    "\t\t\twordsEN = []\n",
    "\t\t\twordsDE = []\n",
    "\t\t\tfor token in cleaned:\n",
    "\t\t\t\tif token in self.englishdic:\n",
    "\t\t\t\t\twordsEN.append(token)\n",
    "\t\t\t\tif token in self.germandic:\n",
    "\t\t\t\t\twordsDE.append(token)\n",
    "\t\t\tif len(wordsEN) > len(wordsDE):\n",
    "\t\t\t\treturn 'english'\n",
    "\t\t\telif len(wordsDE) > len(wordsEN):\n",
    "\t\t\t\treturn 'german'\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn 'english'\n",
    "\t\t\t\n",
    "\t@staticmethod\n",
    "\tdef _getGermanFreqDist():\n",
    "\t\t\"\"\"\n",
    "\t\tWalks through germanfreq.txt, splits the values into terms and frequencies\n",
    "\t\tthen maps them to a dictionary.\n",
    "\t\t\n",
    "\t\tIch\t489637 -> {ich: 489637}\n",
    "\t\tist\t475043 -> {ich: 489637, ist: 475043}\n",
    "\t\tich\t440346 -> {ich: 929983, ist: 475043} - adds to the count of capital 'ich'\n",
    "\t\t\n",
    "\t\t:return: a frequency distribution dictionary of German words\n",
    "\t\t\"\"\"\n",
    "\t\twith open('germanfreq.txt', 'r') as f:\n",
    "\t\t\tfdist = {}\n",
    "\t\t\tlines = f.read().splitlines()\n",
    "\t\t\tfor line in lines:\n",
    "\t\t\t\tparts = line.split()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# List comprehension is necessary because the file is sometimes organize \"term freq\" and sometimes\n",
    "\t\t\t\t# \"freq term\" and it only works because there are no cardinal number terms included in the file.\n",
    "\t\t\t\tterm = [p for p in parts if not p.isdigit()][0]\n",
    "\t\t\t\tfreq = int([p for p in parts if p.isdigit()][0])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# The entries are split into capital and lowercase; here we combine the two.\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tfdist[term] = fdist[term] + freq\n",
    "\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\tfdist[term] = freq\n",
    "\t\t\t\t\t\n",
    "\t\t\treturn fdist\n",
    "\t\t\n",
    "\tdef _getTokens2ids(self):\n",
    "\t\t\"\"\"\n",
    "\t\tIndexes all the tokens and maps them to a list of tweetIDs.\n",
    "\t\t:return: a dictionary visualized as {token: [tweetID1, tweetID2, ...]}\n",
    "\t\t\"\"\"\n",
    "\t\t# For the sake of time and presenting functionality, we're limiting the number\n",
    "\t\t# of tweets that we are indexing.\n",
    "\t\tMAX_DOCS_TO_INDEX = 25\n",
    "\t\ti = 0\n",
    "\t\t\n",
    "\t\ttokens2id = {}\n",
    "\t\t\n",
    "\t\tfor id, doc in self.id2doc.items():\n",
    "\t\t\tdoc = self.clean(doc)\n",
    "\t\t\tlanguage = self._detectLanguage(' '.join(doc))\n",
    "\t\t\t\n",
    "\t\t\t# This print statement is for demonstration purposes \n",
    "\t\t\tprint(language, doc)\n",
    "\n",
    "\t\t\tfor t in doc:\n",
    "\t\t\t\tif language == 'english':\n",
    "\t\t\t\t\t# We are specifically excluding handles and hashtags\n",
    "\t\t\t\t\t# Nor do we want to spellcheck words that are in the dictionary\n",
    "\t\t\t\t\tif t[0] not in ['@', '#'] and t not in self.englishdic:\n",
    "\t\t\t\t\t\toriginal = t\n",
    "\t\t\t\t\t\tt = self.spellCheck(t, language)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Collects corrected words for demonstration purposes\n",
    "\t\t\t\t\t\tif original != t:\n",
    "\t\t\t\t\t\t\tself.correctedTerms += (original, t)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\telif language == 'german':\n",
    "\t\t\t\t\tif t[0] not in ['@', '#'] and t not in self.germandic:\n",
    "\t\t\t\t\t\toriginal = t\n",
    "\t\t\t\t\t\tt = self.spellCheck(t, language)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif original != t:\n",
    "\t\t\t\t\t\t\tself.correctedTerms += (original, t)\n",
    "\n",
    "\t\t\t\tif t in tokens2id.keys():\n",
    "\t\t\t\t\ttokens2id[t].add(id)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# a set is used to avoid multiple entries of the same tweetID\n",
    "\t\t\t\t\ttokens2id[t] = {id}\n",
    "\t\t\t\n",
    "\t\t\t# Break the loop after MAX_DOCS_TO_INDEX iterations\n",
    "\t\t\ti += 1\n",
    "\t\t\tif i >= MAX_DOCS_TO_INDEX:\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\treturn tokens2id\n",
    "\n",
    "\tdef index(self, path):\n",
    "\t\t\"\"\"\n",
    "        1) call the method to read the file in\n",
    "        2) iterate over the original datastructure id2doc which keeps the mapping\n",
    "        of the tweet ids to the actual tweets and do:\n",
    "            2a) preprocessing of the tweets\n",
    "            2b) create a mapping from each token to its postings list (tokens2id)\n",
    "        3) iterate over the just created mapping of tokens to their respective \n",
    "        postings lists (tokens2id) and do:\n",
    "            3a) calculate the size of the postingslist\n",
    "            3b) sort the postings list numerically in ascending order\n",
    "            3c) create a linked list for the postings list\n",
    "            3d) create the Index object with the size of the postings list and\n",
    "            the pointer to the postings list - add to the resulting datastructure \n",
    "        :param path: the path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\t\tself.initLanguageDics('./', filenameGER='germandic-utf8.sec')\n",
    "\t\tself.initId2doc(path)\n",
    "\t\tself._indexPostings(self._getTokens2ids())\n",
    "\n",
    "\tdef _indexPostings(self, tokens2id):\n",
    "\t\t\"\"\"\n",
    "\t\tCreates an `Index` object, which contains a pointer to to the beginning\n",
    "\t\tof a postings list for every key/token in the `tokens2id` dictionary. It\n",
    "\t\tstores this in the master inverted index `self.indices`.\n",
    "\t\t\n",
    "\t\t:param tokens2id: \n",
    "\t\t\"\"\"\n",
    "\t\tfor t, ids in tokens2id.items():\n",
    "\t\t\t# size of the postings list which belongs to token t\n",
    "\t\t\tsize = len(ids)\n",
    "\t\t\t# sort in ascending order\n",
    "\t\t\tids = sorted(ids)\n",
    "\t\t\t# use the first (and smallest) tweetID to be the head node of the \n",
    "\t\t\t# linked list\n",
    "\t\t\tnode = PostingNode(ids[0])\n",
    "\t\t\t# keep reference to the head of the linked list since node variable\n",
    "\t\t\t# is going to be overridden\n",
    "\t\t\tpointer = node\n",
    "\t\t\tfor id in ids[1:]:\n",
    "\t\t\t\t# create further list items\n",
    "\t\t\t\tn = PostingNode(id)\n",
    "\t\t\t\t# and append to the linked list\n",
    "\t\t\t\tnode.next = n\n",
    "\t\t\t\t# step further\n",
    "\t\t\t\tnode = n\n",
    "\t\t\t# create the index object with size of the postings list \n",
    "\t\t\t# and a link to the postings list itself\n",
    "\t\t\ti = Index(size, pointer)\n",
    "\t\t\tself.indices[t] = i\n",
    "\t\t\t\n",
    "\tdef initId2doc(self, path):\n",
    "\t\t\"\"\"\n",
    "        Reads the file in and fills the id2doc datastructure.\n",
    "        :param path: path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\t\twith open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "\t\t\tr = csv.reader(f, delimiter='\\t')\n",
    "\t\t\tfor line in r:\n",
    "\t\t\t\tself.id2doc[line[1]] = line[4]\n",
    "\t\tf.close()\n",
    "\n",
    "\tdef initLanguageDics(self, path_dir, filenameEN='englishdic.sec', filenameGER='germandic.sec'):\n",
    "\t\t\"\"\"\n",
    "\t\tOpens the two dictionary files and creates a Python dict for each one.\n",
    "\t\t\n",
    "\t\t:param path_dir: the directory that contains the two files\n",
    "\t\t:param filenameEN: the name of the English file\n",
    "\t\t:param filenameGER: the name of the German file\n",
    "\t\t\"\"\"\n",
    "\t\tif filenameEN:\n",
    "\t\t\twith open(path_dir + filenameEN, 'r', encoding='utf-8') as f:\n",
    "\t\t\t\tfor word in f.readlines():\n",
    "\t\t\t\t\tself.englishdic.add(word.lower().strip())\n",
    "\t\t\t\tf.close()\n",
    "\n",
    "\t\tif filenameGER:\n",
    "\t\t\twith open(path_dir + filenameGER, 'r', encoding='utf-8') as f:\n",
    "\t\t\t\tfor word in f.readlines():\n",
    "\t\t\t\t\tself.germandic.add(word.lower().strip())\n",
    "\t\t\t\tf.close()\n",
    "\n",
    "\tdef _initSpellCheck(self, lang):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes two `SpellChecker` objects given the path to their dictionary files.\n",
    "\t\t\n",
    "\t\t:param lang: the language of the spell checker \n",
    "\t\t:return: a `SpellChecker` object based on a dictionary in that language\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tif lang == 'english':\n",
    "\t\t\t# `SpellChecker` will use the Brown FreqDist if none is provided\n",
    "\t\t\tfreq_dist = None\n",
    "\t\telif lang == 'german':\n",
    "\t\t\t# For German, we need to create our own.\n",
    "\t\t\tfreq_dist = self._getGermanFreqDist()\n",
    "\t\telse:\n",
    "\t\t\traise Exception(f'{lang} is not a supported language.')\n",
    "\t\t\t\n",
    "\t\treturn SpellChecker(SpellChecker.DEFAULT_DICTIONARIES[lang], fdist=freq_dist).spell_check\n",
    "\t\n",
    "\tdef intersect(self, pointer1, pointer2):\n",
    "\t\t\"\"\"\n",
    "        Computes the intersection for two postings lists.\n",
    "        :param pointer1: first postings list\n",
    "        :param pointer2: second postings list\n",
    "        :return: returns the intersection \n",
    "        \"\"\"\n",
    "\t\t# create temporary head node\n",
    "\t\tnode = PostingNode('tmp')\n",
    "\t\t# keep reference to head node\n",
    "\t\trvalpointer = node\n",
    "\t\twhile pointer1 and pointer2:\n",
    "\t\t\tval1 = pointer1.val\n",
    "\t\t\tval2 = pointer2.val\n",
    "\t\t\t# only append to the linked list if the values are equal\n",
    "\t\t\tif val1 == val2:\n",
    "\t\t\t\tn = PostingNode(val1)\n",
    "\t\t\t\tnode.next = n\n",
    "\t\t\t\tnode = n\n",
    "\t\t\t\tpointer1 = pointer1.next\n",
    "\t\t\t\tpointer2 = pointer2.next\n",
    "\t\t\t# otherwise the postings list with the smaller value \n",
    "\t\t\t# at the current index moves one forward\n",
    "\t\t\telif val1 > val2:\n",
    "\t\t\t\tpointer2 = pointer2.next\n",
    "\t\t\telif val1 < val2:\n",
    "\t\t\t\tpointer1 = pointer1.next\n",
    "\t\t# return from the second element on since the first was the temporary one\n",
    "\t\treturn rvalpointer.next\n",
    "\n",
    "\tdef _query(self, term, lang):\n",
    "\t\t\"\"\"\n",
    "        Internal method to query for one term.\n",
    "        :param: term the word which was queried for \n",
    "        :param: lang the language of the term for spellchecking\n",
    "        :return: returns the Index object of the corresponding query term\n",
    "        \"\"\"\n",
    "\t\tif lang == 'english':\n",
    "\t\t\tif term not in self.englishdic:\n",
    "\t\t\t\tterm = self.spellCheck(term, lang)\n",
    "\t\telif lang == 'german':\n",
    "\t\t\tif term not in self.germandic:\n",
    "\t\t\t\tterm = self.spellCheck(term, lang)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\treturn self.indices[term]\n",
    "\t\texcept KeyError:\n",
    "\t\t\treturn Index(0, PostingNode(''))\n",
    "\n",
    "\tdef query(self, *arg):\n",
    "\t\t\"\"\"\n",
    "        Query method which can take any number of terms as arguments.\n",
    "        It uses the internal _query method to get the postings lists for the single \n",
    "        terms. It calculates the intersection of all postings lists.\n",
    "        :param *arg term arguments\n",
    "        :return: returns a list of tweetIDs which all contain the query terms\n",
    "        \"\"\"\n",
    "\t\tlanguage = self._detectLanguage(' '.join([t for t in arg]))\n",
    "\t\tprint(language)\n",
    "\t\t# [self._query(t, language) for t in arg if t not in self.stop_words]\n",
    "\t\t# sys.exit(0)\n",
    "\n",
    "\t\t# at this point it's a list of Index objects\n",
    "\t\tpointers = [self._query(t, language) for t in arg if t not in self.stop_words]\n",
    "\t\t# here the Index objects get sorted by the size of the \n",
    "\t\t# postings list they point to\n",
    "\t\tpointers = sorted(pointers, key=lambda i: i.size)\n",
    "\t\t# here it becomes a list of pointers to the postings lists\n",
    "\t\tpointers = [i.pointer2postingsList for i in pointers]\n",
    "\t\t# first pointer\n",
    "\t\tintersection = pointers[0]\n",
    "\t\t# step through the pointers\n",
    "\t\tfor p in pointers[1:]:\n",
    "\t\t\t# intersection between the new postings list and the so far\n",
    "\t\t\t# computed intersection\n",
    "\t\t\tintersection = self.intersect(intersection, p)\n",
    "\t\t\t# if at any point the intersection is empty there is \n",
    "\t\t\t# no need to continue\n",
    "\t\t\tif not intersection:\n",
    "\t\t\t\treturn []\n",
    "\t\t# convert the resulting intersection to a normal list\n",
    "\t\trval = []\n",
    "\t\tpointer = intersection\n",
    "\t\twhile pointer:\n",
    "\t\t\trval.append(pointer.val)\n",
    "\t\t\tpointer = pointer.next\n",
    "\n",
    "\t\treturn rval\n",
    "\t\t\n",
    "\tdef spellCheck(self, term, lang):\n",
    "\t\t\"\"\"Runs the relevant spellchecker method.\"\"\"\n",
    "\t\treturn {'english': self.engSpellCheck,\n",
    "\t\t        'german': self.gerSpellCheck}[lang](term)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"The number of tokens in the inverted index.\"\"\"\n",
    "\t\treturn len(self.indices.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Index\n",
    "Because the algorithm takes a significant amount of time to run, we have decided to index only the first 50 tweets, which should suffice to demonstrate the spellcheck and language detection algorithms.\n",
    "\n",
    "In the cell below are the 50 tweets represented as lists of tokenized terms, and printed directly below them are the results of the language detection algorithm. We are relying on the assumption that tweets are generally written in a single language, which we then use to spellcheck the content of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['@knakatani', '@chikonjugular', '@joofford', '@steveblogs', 'says', 'lifetime', 'risk', 'cervical', 'cancer', 'japan', 'means', 'hpv', 'endemic', 'japan', 'screening', 'working', 'well']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['@fischerkurt', 'lady', 'whats', 'tumor', '#kippcharts']\ngerman ['@kingsofmetal', 'diagnoseverdacht', 'nunmal', 'schwer', 'gerade', 'hausarzt', 'blutbild', 'meist', 'sehen', 'gerade', 'hormone', 'überprüft', 'erklärbare', 'gewichtseinlagerungen', 'ja', 'wasser', 'fett', 'kind', 'tumor']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german ['@germanletsplay', '@quentin', '@lopoopl', '@levanni', '@igeloe', '@annelle', 'glückwunsch']\nenglish ['interesting', 'pcr', 'rate', 'major', 'centers', 'authors', 'argue', 'treatment', 'compliance', 'major', 'centers', 'see', 'database', 'think', 'rather', 'due', 'earlier', 'detection', 'smaller', 'tumors', 'pcr', 'look', 'deeper', '#crcsm']\nenglish ['new', 'nanobots', 'kill', 'cancerous', 'tumors', 'cutting', 'blood', 'supply', '#digitaleconomy', 'february', 'pm']\ngerman ['rip', 'salem', 'aufgrund', 'tumors', 'eingeschläfert']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['cancerfighting', 'nanorobots', 'programmed', 'seek', 'destroy', 'tumors', 'study', 'shows', 'first', 'applications', 'dna', 'origami', 'nanomedicine', 'nanoroboter', 'schrumpfen', 'tumore', '#medtech']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german ['@riptear', 'tumorsdat', 'leg', 'straight', 'mccain']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['quote', 'one', 'statement', 'cancers']\nenglish ['@joyannreid', '@pampylu', 'wrong', 'think', 'trump', 'probleme', 'mere', 'small', 'symptom', 'systematic', 'cancer', 'ask', 'antigunkids', 'go', 'look', 'failures', 'us', 'democracy']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['erstmal', 'nen', 'anti', 'cancer', 'stick', 'lunge', 'reintherapieren']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['#usa', '#upi', '#news', 'broadcast', '#emetnewspress', 'obesity', 'may', 'cause', 'sudden', 'cardiac', 'arrest', 'young', 'people', 'study', 'says', 'obesity', 'high', 'blood', 'pressure', 'may', 'play', 'much', 'greater', 'role', 'sudden', 'cardiac', 'arrest', 'among', 'young', 'people', 'previously', 'thought', 'ne']\nenglish ['leseempfehlung', 'extraordinary', 'correlation', 'obesity', 'social', 'inequality']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['@fazefuzzface', 'welcome', 'obesity']\nenglish ['@isonnylucas', 'thats', 'exactly', 'point', 'whataboutism', 'dont', 'want', 'face', 'problem', 'point', 'worse', 'problem', 'bfollowing', 'logic', 'yes', 'opioid', 'crisis', 'bad', 'obesity', 'affects', 'way', 'children', 'many', 'deathslike', 'never', 'solve', 'problem']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['obese', 'adult', 'free', 'online', 'mobile', 'sex']\nenglish ['obese', 'ebony', 'porn', 'carrie', 'fisher', 'naked', 'pictures']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['@obesetobeast', 'sad', 'days', 'cant', 'get', 'days', 'stuff', 'like', 'reducing', 'damage', 'stuff', 'doesnt', 'move', 'way']\nenglish ['fat', 'obese', 'sex', 'pictures', 'big', 'booty', 'asians', 'fucking']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['obese', 'porn', 'gallery', 'overcome', 'sex', 'addiction']\ngerman ['amateur', 'downblouse', 'videos', 'get', 'hiv', 'oral', 'sex']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german ['syphilis', 'kommt', 'wiederja', 'preptherapie', 'hiv', 'übertragungsinfektionsrisiko', 'nahezu', 'null', 'senkenaber', 'syphilis', 'kleine', 'ficker', 'schon', 'längst', 'dasyphilis', 'tut', 'anfang', 'weh', 'ende', 'beißt', 'arsch']\n"
     ]
    }
   ],
   "source": [
    "twitterIR = TwitterIR()\n",
    "twitterIR.index('tweets.csv')"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 3,
   "source": [
    "Corrected Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original, corrected in twitterIR.correctedTerms:\n",
    "\tprint(f'{original} was corrected to {corrected}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Index\n",
    "\n",
    "Below we will attempt to query the German terms 'Blutbild' and 'schwer', but by a slip of the finger we accidentally query the non-word 'blutbilt' and the correctly spelt 'schwer'. The `query` method prints the language it identified from the query for our benefit and then prints the TweetIDs of the results of the `query` method. We then fetch the tweet itself and print it for those of us who did not memorize every tweet and its corresponding ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['965695626150326273']\n@Kings_of_Metal Ohne Diagnoseverdacht ist es nunmal schwer, gerade für einen Hausarzt. Am Blutbild kann man meist nicht viel sehen, gerade wenn man nicht auch die Hormone überprüft. Nicht erklärbare Gewichtseinlagerungen können ja alles sein, von Wasser, Fett, Kind bis hin zum Tumor.\n"
     ]
    }
   ],
   "source": [
    "query_result = twitterIR.query('blutbilt', 'schwer')\n",
    "print(query_result)\n",
    "print(twitterIR.id2doc[query_result[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In English Please\n",
    "\n",
    "Let's give it an English query: say, all tweets containing the words 'major', 'centers', and 'authors'. But alas! We are bested by the unforgiving cryptology that is English orthographic tradition and we instead query 'major', 'senters', and 'authers'. Luckily our spellchecker is there to bail us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['965672579133566980']\nInteresting. ⬆️ pCR rate at major centers. Authors argue with ⬆️ treatment compliance at major centers. We see the same in our database. I think it’s rather due to earlier detection, smaller tumors ➡️ more pCR. Will look deeper into this. #crcsm https://t.co/QfL5g2Z5u9\n"
     ]
    }
   ],
   "source": [
    "query_result = twitterIR.query('major', 'senters', 'authers')\n",
    "print(query_result)\n",
    "print(twitterIR.id2doc[query_result[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
