{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from spell_checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    \"\"\"\n",
    "    This data structure is the value of the indices dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, pointer2postingsList):\n",
    "        # size of the postings list\n",
    "        \n",
    "        self.size = size\n",
    "        # pointer to the head of the postings list\n",
    "        self.pointer2postingsList = pointer2postingsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingNode:\n",
    "    \"\"\"\n",
    "    Linked list for the postings list\n",
    "    \"\"\"\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker(object):\n",
    "    DEFAULT_DICTIONARIES = {'english': open('englishdic.sec', 'r').read().splitlines(),\n",
    "                            'german': open('germandic-utf8.sec', 'r').read().splitlines()}\n",
    "\n",
    "    def __init__(self, lang_vocab: list, fdist: dict = None, max_edit_distance: int = 2):\n",
    "        \"\"\"\n",
    "        Creates a `SpellChecker` object from a dictionary and a frequency distribution. The principle\n",
    "        method is `spell_check`. Every other method is called in calling it. To best understand this\n",
    "        class, start there and work your way through the call sequence: `spell_check` calls\n",
    "        `candidates` which in turns calls `known` and the edit distance methods.\n",
    "\n",
    "        :param lang_vocab: a list of words in a language's vocabulary\n",
    "        :param fdist: a dictionary-like frequency distribution from a large corpus;\n",
    "                if none is provided and the class default English dictionary is supplied to\n",
    "                `lang_vocab`, the Brown corpus is imported and used\n",
    "        :param max_edit_distance: the maximum edit distance at which words will still be considered\n",
    "        \"\"\"\n",
    "        # At present, sticking with the German alphabet even for English\n",
    "        self.alphabet = 'aäbcdefghijklmnoöpqrsßtuüvwxyz'\n",
    "        # Key:Value pair of alphabet letters and lists of words beginning with those letters\n",
    "        # in the provided list of dictionary terms to decrease the time it takes to do dictionary lookups.\n",
    "        self.lang_vocab = {letter: [word.lower() for word in lang_vocab \\\n",
    "                                    if word.lower().startswith(letter)] for letter in self.alphabet}\n",
    "        self.fdist = fdist\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "\n",
    "        # If no fdist provided and `lang_vocab` is default English, use the Brown news corpus'.\n",
    "        # In case you don't have a corpus big enough to create a strong frequency distribution\n",
    "        if self.fdist is None:\n",
    "            if lang_vocab == SpellChecker.DEFAULT_DICTIONARIES['english']:\n",
    "                from nltk.corpus import brown\n",
    "                from nltk import FreqDist\n",
    "\n",
    "                self.fdist = FreqDist(w.lower() for w in brown.words(categories='news'))\n",
    "\n",
    "            else:\n",
    "                raise TypeError('No frequency distribution index provided.')\n",
    "\n",
    "    def candidates(self, word: str) -> set:\n",
    "        \"\"\"\n",
    "        Returns words within an edit distance of 2 in a ranked order, only generating words\n",
    "        if there are no results from the previous method. If the word does not begin with\n",
    "        a letter in `self.alphabet`, it is returned immediately as it was given.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return (self.known([word.lower()]) or                     # word if it is known\n",
    "                    self.known(self.edit_distance1(word.lower())) or  # known words with edit distance 1\n",
    "                    self.known(self.edit_distance2(word.lower())) or  # known words with edit distance 2\n",
    "                    [word])                                           # word, unknown\n",
    "        except KeyError:\n",
    "            return [word]\n",
    "\n",
    "    def edit_distance1(self, word: str) -> set:\n",
    "        \"\"\"\n",
    "        Thanks to Peter Norvig (http://norvig.com/spell-correct.html)\n",
    "\n",
    "        Creates all the possible letter combinations that can be made\n",
    "        with an edit distance of 1 to the word.\n",
    "\n",
    "        splits = all ways of dividing the word, e.g. \n",
    "            'word' -> ('w', 'ord'); useful for making changes\n",
    "        deletions = all ways of removing a single letter, e.g.\n",
    "            'word'-> 'ord'\n",
    "        transpositions = all ways of swapping two letters immediately\n",
    "            adjacent to one another, e.g. 'word' -> 'owrd'\n",
    "        replacements = all ways of replacing a letter with another\n",
    "            letter, e.g. 'word' -> 'zord'\n",
    "        insertions = all ways of inserting a letter at any point in the\n",
    "            word, e.g. 'word' -> 'wgord'\n",
    "\n",
    "        :param str word: the relevant word\n",
    "        :return: a set of terms with an edit distance of 1 from the word\n",
    "        :rtype: set\n",
    "        \"\"\"\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletions = [left + right[1:] for left, right in splits if right]\n",
    "        transpositions = [left + right[1] + right[0] + right[2:]\n",
    "                          for left, right in splits if len(right) > 1]\n",
    "        replacements = [left + letter + right[1:] for left, right\n",
    "                        in splits if right for letter in self.alphabet]\n",
    "        insertions = [left + letter + right for left, right in splits\n",
    "                      for letter in self.alphabet]\n",
    "\n",
    "        return set(deletions + transpositions + replacements + insertions)\n",
    "\n",
    "    def edit_distance2(self, word: str) -> set:\n",
    "        \"\"\"Simply runs edit_distance1 on every result from edit_distance1(word)\"\"\"\n",
    "        return set(edit2 for edit in self.edit_distance1(word) for edit2\n",
    "                   in self.edit_distance1(edit))\n",
    "\n",
    "    def edit_distanceN(self, word: str) -> set:\n",
    "        # FIXME\n",
    "        \"\"\"Runs `edit_distance1` on the results of `edit_distance1` n times.\"\"\"\n",
    "        ret_val = set(word)\n",
    "\n",
    "        for _ in range(self.max_edit_distance):\n",
    "            for val in ret_val:\n",
    "                ret_val = ret_val | self.edit_distance1(val)\n",
    "\n",
    "        return ret_val\n",
    "\n",
    "    def in_dictionary(self, word: str) -> bool:\n",
    "        \"\"\"Returns whether the word is in the dictionary.\"\"\"\n",
    "        try:\n",
    "            return word in self.lang_vocab[word[0].lower()]\n",
    "        except KeyError:\n",
    "            return False\n",
    "\n",
    "    def known(self, words: list) -> set:\n",
    "        \"\"\"\n",
    "        Walks through words in a list, checks them against `lang_vocab`,\n",
    "        and returns a set of those that match.\n",
    "        \"\"\"\n",
    "        return set(w for w in words if len(w) > 1 and self.in_dictionary(w))\n",
    "\n",
    "    def spell_check(self, word: str) -> str:\n",
    "        \"\"\"Chooses the most likely word in a set of candidates based on `word_probability`.\"\"\"\n",
    "        return max(self.candidates(word), key=self.word_probability)\n",
    "\n",
    "    def word_probability(self, word: str) -> int:\n",
    "        \"\"\"Divides the frequency of a word by overall token count.\"\"\"\n",
    "        try:\n",
    "            return self.fdist[word.lower()] / len(self.fdist.keys())\n",
    "        except KeyError:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterIR(object):\n",
    "    \"\"\"\n",
    "    Main Class for the information retrieval task.\n",
    "    \"\"\"\n",
    "    __slots__ = 'id2doc', 'tokenizer', 'unicodes2remove', 'indices', \\\n",
    "                'urlregex', 'punctuation', 'emojis', 'stop_words', \\\n",
    "                'engSpellCheck', 'gerSpellCheck', 'correctedTerms'\n",
    "\n",
    "    def __init__(self):\n",
    "        # the original mapping from the id's to the tweets, \n",
    "        # which is kept until the end to index the tweets\n",
    "        self.id2doc = {}\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        # bunch of punctuation unicodes which are not in 'string.punctuation'\n",
    "        self.unicodes2remove = [\n",
    "            # all kinds of quotes\n",
    "            u'\\u2018', u'\\u2019', u'\\u201a', u'\\u201b', u'\\u201c', \\\n",
    "            u'\\u201d', u'\\u201e', u'\\u201f', u'\\u2014',\n",
    "            # all kinds of hyphens\n",
    "            u'\\u002d', u'\\u058a', u'\\u05be', u'\\u1400', u'\\u1806', \\\n",
    "            u'\\u2010', u'\\u2011', u'\\u2012', u'\\u2013',\n",
    "            u'\\u2014', u'\\u2015', u'\\u2e17', u'\\u2e1a', u'\\u2e3a', \\\n",
    "            u'\\u2e3b', u'\\u2e40', u'\\u301c', u'\\u3030',\n",
    "            u'\\u30a0', u'\\ufe31', u'\\ufe32', u'\\ufe58', u'\\ufe63', \\\n",
    "            u'\\uff0d', u'\\u00b4'\n",
    "        ]\n",
    "        # the resulting data structure which has the tokens as keys\n",
    "        # and the Index objects as values\n",
    "        self.indices = {}\n",
    "        # regex to match urls (taken from the web)\n",
    "        self.urlregex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]'\n",
    "                                   '|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        # keep @ to be able to recognize usernames\n",
    "        self.punctuation = string.punctuation.replace('@', '') + \\\n",
    "                           ''.join(self.unicodes2remove)\n",
    "        self.punctuation = self.punctuation.replace('#', '')\n",
    "        self.punctuation = self.punctuation.replace('…', '')\n",
    "        # a bunch of emoji unicodes\n",
    "        self.emojis = ''.join(emoji.UNICODE_EMOJI)\n",
    "        self.emojis = self.emojis.replace('#', '')\n",
    "        # combined english and german stop words\n",
    "        self.stop_words = set(stopwords.words('english') + stopwords.words('german'))\n",
    "        self.engSpellCheck = self._initSpellCheck('english')\n",
    "        self.gerSpellCheck = self._initSpellCheck('german')\n",
    "        self.correctedTerms = []    # For demonstration purposes only\n",
    "\n",
    "    def clean(self, s):\n",
    "        \"\"\"\n",
    "        Normalizes a string (tweet) by removing the urls, punctuation, digits,\n",
    "        emojis, by putting everything to lowercase and removing the\n",
    "        stop words. Tokenization is performed aswell.\n",
    "\n",
    "        :param s the string (tweet) to clean\n",
    "        :return: returns a list of cleaned tokens\n",
    "        \"\"\"\n",
    "        s = ' '.join(s.replace('[NEWLINE]', '').split())\n",
    "        s = ' '.join(s.replace('…', '...').split())\n",
    "        s = self.urlregex.sub('', s).strip()\n",
    "        s = s.translate(str.maketrans('', '', self.punctuation + string.digits \\\n",
    "                                      + self.emojis)).strip()\n",
    "        s = ' '.join(s.split())\n",
    "        s = s.lower()\n",
    "        s = self.tokenizer.tokenize(s)\n",
    "        s = [w for w in s if w not in self.stop_words]\n",
    "        return s\n",
    "\n",
    "    def _detectLanguage(self, context):\n",
    "        \"\"\"\n",
    "        Detects the language of a tweet based on a hierarchy of criteria:\n",
    "        1. the number of stopwords from each language in a tweet\n",
    "        2. the number of \"normal\" words from each language in a tweet\n",
    "        3. the more common language, in this case English\n",
    "        :param context: \n",
    "        :return: the determined language of the tweet\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(context)\n",
    "        stopsEN = [token for token in tokens if token in stopwords.words('english')]\n",
    "        stopsDE = [token for token in tokens if token in stopwords.words('german')]\n",
    "\n",
    "        # Chooses a language based on the number of stopwords\n",
    "        if len(stopsEN) > len(stopsDE):\n",
    "            return 'english'\n",
    "        elif len(stopsDE) > len(stopsEN):\n",
    "            return 'german'\n",
    "        # If that comparison isn't conclusive, it compares the number of words\n",
    "        # that exist in the respective dictionaries.\n",
    "        else:\n",
    "            cleaned = self.clean(context)\n",
    "\n",
    "            wordsEN = [token for token in cleaned if self.engSpellCheck.in_dictionary(token)]\n",
    "            wordsDE = [token for token in cleaned if self.gerSpellCheck.in_dictionary(token)]\n",
    "\n",
    "            if len(wordsEN) > len(wordsDE):\n",
    "                return 'english'\n",
    "            elif len(wordsDE) > len(wordsEN):\n",
    "                return 'german'\n",
    "            # If it still cannot decide, it defaults to the more common language: English\n",
    "            else:\n",
    "                return 'english'\n",
    "\n",
    "    @staticmethod\n",
    "    def _getGermanFreqDist():\n",
    "        \"\"\"\n",
    "        Walks through germanfreq.txt, splits the values into terms and frequencies\n",
    "        then maps them to a dictionary.\n",
    "\n",
    "        Ich\t489637 -> {ich: 489637}\n",
    "        ist\t475043 -> {ich: 489637, ist: 475043}\n",
    "        ich\t440346 -> {ich: 929983, ist: 475043} - adds to the count of capital 'ich'\n",
    "\n",
    "        :return: a frequency distribution dictionary of German words\n",
    "        \"\"\"\n",
    "        with open('germanfreq.txt', 'r') as f:\n",
    "            fdist = {}\n",
    "            lines = f.read().splitlines()\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "\n",
    "                # List comprehension is necessary because the file is sometimes organize \"term freq\" and sometimes\n",
    "                # \"freq term\" and it only works because there are no cardinal number terms included in the file.\n",
    "                term = [p for p in parts if not p.isdigit()][0]\n",
    "                freq = int([p for p in parts if p.isdigit()][0])\n",
    "\n",
    "                # The entries are split into capital and lowercase; here we combine the two.\n",
    "                try:\n",
    "                    fdist[term] = fdist[term] + freq\n",
    "                except KeyError:\n",
    "                    fdist[term] = freq\n",
    "\n",
    "            return fdist\n",
    "\n",
    "    def _getTokens2ids(self):\n",
    "        \"\"\"\n",
    "        Indexes all the tokens and maps them to a list of tweetIDs.\n",
    "        :return: a dictionary visualized as {token: [tweetID1, tweetID2, ...]}\n",
    "        \"\"\"\n",
    "        # For the sake of time and presenting functionality, we're limiting the number\n",
    "        # of tweets that we are indexing.\n",
    "        MAX_DOCS_TO_INDEX = 25\n",
    "        i = 0\n",
    "\n",
    "        tokens2id = {}\n",
    "\n",
    "        for id, doc in self.id2doc.items():\n",
    "            doc = self.clean(doc)\n",
    "            language = self._detectLanguage(' '.join(doc))\n",
    "\n",
    "            # This print statement is for demonstration purposes \n",
    "            print(language, doc)\n",
    "\n",
    "            for t in doc:\n",
    "                if language == 'english':\n",
    "                    # We are specifically excluding handles and hashtags\n",
    "                    # Nor do we want to spellcheck words that are in the dictionary\n",
    "                    if t[0] not in ['@', '#'] and not self.engSpellCheck.in_dictionary(t):\n",
    "                        original = t\n",
    "                        t = self.spellCheck(t, language)\n",
    "\n",
    "                        # Collects corrected words for demonstration purposes\n",
    "                        if original != t:\n",
    "                            self.correctedTerms.append((original, t))\n",
    "\n",
    "                elif language == 'german':\n",
    "                    if t[0] not in ['@', '#'] and not self.gerSpellCheck.in_dictionary(t):\n",
    "                        original = t\n",
    "                        t = self.spellCheck(t, language)\n",
    "\n",
    "                        if original != t:\n",
    "                            self.correctedTerms.append((original, t))\n",
    "\n",
    "                if t in tokens2id.keys():\n",
    "                    tokens2id[t].add(id)\n",
    "                else:\n",
    "                    # a set is used to avoid multiple entries of the same tweetID\n",
    "                    tokens2id[t] = {id}\n",
    "\n",
    "            # Break the loop after MAX_DOCS_TO_INDEX iterations\n",
    "            i += 1\n",
    "            if i >= MAX_DOCS_TO_INDEX:\n",
    "                break\n",
    "\n",
    "        return tokens2id\n",
    "\n",
    "    def index(self, path):\n",
    "        \"\"\"\n",
    "        1) call the method to read the file in\n",
    "        2) iterate over the original datastructure id2doc which keeps the mapping\n",
    "        of the tweet ids to the actual tweets and do:\n",
    "            2a) preprocessing of the tweets\n",
    "            2b) create a mapping from each token to its postings list (tokens2id)\n",
    "        3) iterate over the just created mapping of tokens to their respective \n",
    "        postings lists (tokens2id) and do:\n",
    "            3a) calculate the size of the postingslist\n",
    "            3b) sort the postings list numerically in ascending order\n",
    "            3c) create a linked list for the postings list\n",
    "            3d) create the Index object with the size of the postings list and\n",
    "            the pointer to the postings list - add to the resulting datastructure \n",
    "        :param path: the path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.initId2doc(path)\n",
    "        self._indexPostings(self._getTokens2ids())\n",
    "\n",
    "    def _indexPostings(self, tokens2id):\n",
    "        \"\"\"\n",
    "        Creates an `Index` object, which contains a pointer to to the beginning\n",
    "        of a postings list for every key/token in the `tokens2id` dictionary. It\n",
    "        stores this in the master inverted index `self.indices`.\n",
    "\n",
    "        :param tokens2id: \n",
    "        \"\"\"\n",
    "        for t, ids in tokens2id.items():\n",
    "            # size of the postings list which belongs to token t\n",
    "            size = len(ids)\n",
    "            # sort in ascending order\n",
    "            ids = sorted(ids)\n",
    "            # use the first (and smallest) tweetID to be the head node of the \n",
    "            # linked list\n",
    "            node = PostingNode(ids[0])\n",
    "            # keep reference to the head of the linked list since node variable\n",
    "            # is going to be overridden\n",
    "            pointer = node\n",
    "            for id in ids[1:]:\n",
    "                # create further list items\n",
    "                n = PostingNode(id)\n",
    "                # and append to the linked list\n",
    "                node.next = n\n",
    "                # step further\n",
    "                node = n\n",
    "            # create the index object with size of the postings list \n",
    "            # and a link to the postings list itself\n",
    "            i = Index(size, pointer)\n",
    "            self.indices[t] = i\n",
    "\n",
    "    def initId2doc(self, path):\n",
    "        \"\"\"\n",
    "        Reads the file in and fills the id2doc datastructure.\n",
    "        :param path: path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "            r = csv.reader(f, delimiter='\\t')\n",
    "            for line in r:\n",
    "                self.id2doc[line[1]] = line[4]\n",
    "        f.close()\n",
    "\n",
    "    def _initSpellCheck(self, lang):\n",
    "        \"\"\"\n",
    "        Initializes two `SpellChecker` objects given the path to their dictionary files.\n",
    "\n",
    "        :param lang: the language of the spell checker \n",
    "        :return: a `SpellChecker` object based on a dictionary in that language\n",
    "        \"\"\"\n",
    "\n",
    "        if lang == 'english':\n",
    "            # `SpellChecker` will use the Brown FreqDist if none is provided\n",
    "            freq_dist = None\n",
    "        elif lang == 'german':\n",
    "            # For German, we need to create our own.\n",
    "            freq_dist = self._getGermanFreqDist()\n",
    "        else:\n",
    "            raise Exception(f'{lang} is not a supported language.')\n",
    "\n",
    "        return SpellChecker(SpellChecker.DEFAULT_DICTIONARIES[lang], fdist=freq_dist)\n",
    "\n",
    "    def intersect(self, pointer1, pointer2):\n",
    "        \"\"\"\n",
    "        Computes the intersection for two postings lists.\n",
    "        :param pointer1: first postings list\n",
    "        :param pointer2: second postings list\n",
    "        :return: returns the intersection \n",
    "        \"\"\"\n",
    "        # create temporary head node\n",
    "        node = PostingNode('tmp')\n",
    "        # keep reference to head node\n",
    "        rvalpointer = node\n",
    "        while pointer1 and pointer2:\n",
    "            val1 = pointer1.val\n",
    "            val2 = pointer2.val\n",
    "            # only append to the linked list if the values are equal\n",
    "            if val1 == val2:\n",
    "                n = PostingNode(val1)\n",
    "                node.next = n\n",
    "                node = n\n",
    "                pointer1 = pointer1.next\n",
    "                pointer2 = pointer2.next\n",
    "            # otherwise the postings list with the smaller value \n",
    "            # at the current index moves one forward\n",
    "            elif val1 > val2:\n",
    "                pointer2 = pointer2.next\n",
    "            elif val1 < val2:\n",
    "                pointer1 = pointer1.next\n",
    "        # return from the second element on since the first was the temporary one\n",
    "        return rvalpointer.next\n",
    "\n",
    "    def _query(self, term, lang):\n",
    "        \"\"\"\n",
    "        Internal method to query for one term.\n",
    "        :param: term the word which was queried for \n",
    "        :param: lang the language of the term for spellchecking\n",
    "        :return: returns the Index object of the corresponding query term\n",
    "        \"\"\"\n",
    "        if lang == 'english':\n",
    "            if not self.engSpellCheck.in_dictionary(term):\n",
    "                term = self.spellCheck(term, lang)\n",
    "        elif lang == 'german':\n",
    "            if not self.gerSpellCheck.in_dictionary(term):\n",
    "                term = self.spellCheck(term, lang)\n",
    "\n",
    "        try:\n",
    "            return self.indices[term]\n",
    "        except KeyError:\n",
    "            return Index(0, PostingNode(''))\n",
    "\n",
    "    def query(self, *arg):\n",
    "        \"\"\"\n",
    "        Query method which can take any number of terms as arguments.\n",
    "        It uses the internal _query method to get the postings lists for the single \n",
    "        terms. It calculates the intersection of all postings lists.\n",
    "        :param *arg term arguments\n",
    "        :return: returns a list of tweetIDs which all contain the query terms\n",
    "        \"\"\"\n",
    "        language = self._detectLanguage(' '.join([t for t in arg]))\n",
    "        print(language)  # For demonstration\n",
    "\n",
    "        # at this point it's a list of Index objects\n",
    "        pointers = [self._query(t, language) for t in arg if t not in self.stop_words]\n",
    "        # here the Index objects get sorted by the size of the \n",
    "        # postings list they point to\n",
    "        pointers = sorted(pointers, key=lambda i: i.size)\n",
    "        # here it becomes a list of pointers to the postings lists\n",
    "        pointers = [i.pointer2postingsList for i in pointers]\n",
    "        # first pointer\n",
    "        intersection = pointers[0]\n",
    "        # step through the pointers\n",
    "        for p in pointers[1:]:\n",
    "            # intersection between the new postings list and the so far\n",
    "            # computed intersection\n",
    "            intersection = self.intersect(intersection, p)\n",
    "            # if at any point the intersection is empty there is \n",
    "            # no need to continue\n",
    "            if not intersection:\n",
    "                return []\n",
    "        # convert the resulting intersection to a normal list\n",
    "        rval = []\n",
    "        pointer = intersection\n",
    "        while pointer:\n",
    "            rval.append(pointer.val)\n",
    "            pointer = pointer.next\n",
    "\n",
    "        return rval\n",
    "\n",
    "    def spellCheck(self, term, lang):\n",
    "        \"\"\"Runs the relevant spellchecker method.\"\"\"\n",
    "        return {'english': self.engSpellCheck,\n",
    "                'german': self.gerSpellCheck}[lang].spell_check(term)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The number of tokens in the inverted index.\"\"\"\n",
    "        return len(self.indices.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Index\n",
    "Because the algorithm takes a significant amount of time to run, we have decided to index only the first 25 tweets, which should suffice to demonstrate the `spellcheck` and `_detectLanguage` algorithms.\n",
    "\n",
    "In the cell below are the 25 tweets represented as lists of tokenized terms, and printed adjacent to them are the results of the language detection algorithm. We are relying — however haphazardly — on the assumption that tweets (and queries) are generally written in a single language, which we then use to spellcheck the content of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['@knakatani', '@chikonjugular', '@joofford', '@steveblogs', 'says', 'lifetime', 'risk', 'cervical', 'cancer', 'japan', 'means', 'hpv', 'endemic', 'japan', 'screening', 'working', 'well']\n",
      "english ['@fischerkurt', 'lady', 'whats', 'tumor', '#kippcharts']\n",
      "german ['@kingsofmetal', 'diagnoseverdacht', 'nunmal', 'schwer', 'gerade', 'hausarzt', 'blutbild', 'meist', 'sehen', 'gerade', 'hormone', 'überprüft', 'erklärbare', 'gewichtseinlagerungen', 'ja', 'wasser', 'fett', 'kind', 'tumor']\n",
      "german ['@germanletsplay', '@quentin', '@lopoopl', '@levanni', '@igeloe', '@annelle', 'glückwunsch']\n",
      "english ['interesting', 'pcr', 'rate', 'major', 'centers', 'authors', 'argue', 'treatment', 'compliance', 'major', 'centers', 'see', 'database', 'think', 'rather', 'due', 'earlier', 'detection', 'smaller', 'tumors', 'pcr', 'look', 'deeper', '#crcsm']\n",
      "english ['new', 'nanobots', 'kill', 'cancerous', 'tumors', 'cutting', 'blood', 'supply', '#digitaleconomy', 'february', 'pm']\n",
      "german ['rip', 'salem', 'aufgrund', 'tumors', 'eingeschläfert']\n",
      "english ['cancerfighting', 'nanorobots', 'programmed', 'seek', 'destroy', 'tumors', 'study', 'shows', 'first', 'applications', 'dna', 'origami', 'nanomedicine', 'nanoroboter', 'schrumpfen', 'tumore', '#medtech']\n",
      "german ['@riptear', 'tumorsdat', 'leg', 'straight', 'mccain']\n",
      "english ['quote', 'one', 'statement', 'cancers']\n"
     ]
    }
   ],
   "source": [
    "twitterIR = TwitterIR()\n",
    "twitterIR.index('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hpv' -> 'he'.\n",
      "'nunmal' -> 'neunmal'.\n",
      "'pcr' -> 'per'.\n",
      "'pcr' -> 'per'.\n",
      "'pm' -> 'am'.\n",
      "'rip' -> 'rap'.\n",
      "'nanorobots' -> 'nanobots'.\n",
      "'applications' -> 'application'.\n",
      "'tumore' -> 'tumored'.\n",
      "'cancers' -> 'cancer'.\n"
     ]
    }
   ],
   "source": [
    "for original, corrected in twitterIR.correctedTerms:\n",
    "\tprint(f\"'{original}' -> '{corrected}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Index\n",
    "### German\n",
    "\n",
    "Below we attempt to query the German terms 'Blutbild' and 'schwer', but by a slip of the finger we accidentally query the non-word 'blutbilt'. The `query` method prints out what it determines to be the language of the query and then prints the relevant TweetIDs. We then fetch the tweet itself and print it for those of us who have not memorized every tweet and its corresponding ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german\n",
      "['965695626150326273']\n",
      "@Kings_of_Metal Ohne Diagnoseverdacht ist es nunmal schwer, gerade für einen Hausarzt. Am Blutbild kann man meist nicht viel sehen, gerade wenn man nicht auch die Hormone überprüft. Nicht erklärbare Gewichtseinlagerungen können ja alles sein, von Wasser, Fett, Kind bis hin zum Tumor.\n"
     ]
    }
   ],
   "source": [
    "query_result = twitterIR.query('blutbilt', 'schwer')\n",
    "print(query_result)\n",
    "print(twitterIR.id2doc[query_result[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In English Please\n",
    "\n",
    "Let's give it an English query: say, all tweets containing the words 'major', 'centers', and 'authors'. But alas! We are bested by the unforgiving cryptology that is English orthographic tradition and we instead query 'major', 'senters', and 'authers'. Luckily our spellchecker is there to bail us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n",
      "['965672579133566980']\n",
      "Interesting. ⬆️ pCR rate at major centers. Authors argue with ⬆️ treatment compliance at major centers. We see the same in our database. I think it’s rather due to earlier detection, smaller tumors ➡️ more pCR. Will look deeper into this. #crcsm https://t.co/QfL5g2Z5u9\n"
     ]
    }
   ],
   "source": [
    "query_result = twitterIR.query('major', 'senters', 'authers')\n",
    "print(query_result)\n",
    "print(twitterIR.id2doc[query_result[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
