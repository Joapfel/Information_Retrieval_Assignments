{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from spell_checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    \"\"\"\n",
    "    This data structure is the value of the indices dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, pointer2postingsList):\n",
    "        #size of the postings list\n",
    "        self.size = size\n",
    "        #pointer to the head of the postings list\n",
    "        self.pointer2postingsList = pointer2postingsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingNode:\n",
    "    \"\"\"\n",
    "    Linked list for the postings list\n",
    "    \"\"\"\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterIR(object):\n",
    "    \"\"\"\n",
    "    Main Class for the information retrieval task.\n",
    "    \"\"\"\n",
    "    __slots__ = 'id2doc', 'tokenizer', 'unicodes2remove', 'indices', \\\n",
    "    'urlregex', 'punctuation', 'emojis', 'stop_words', 'englishdic', 'germandic', \\\n",
    "    'engSpellCheck', 'gerSpellCheck'\n",
    "\n",
    "    def __init__(self):\n",
    "        #the original mapping from the id's to the tweets, \n",
    "        #which is kept until the end to index the tweets\n",
    "        self.id2doc = {}\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        #bunch of punctuation unicodes which are not in 'string.punctuation'\n",
    "        self.unicodes2remove = [\n",
    "            #all kinds of quotes\n",
    "            u'\\u2018', u'\\u2019', u'\\u201a', u'\\u201b', u'\\u201c',\\\n",
    "            u'\\u201d', u'\\u201e', u'\\u201f', u'\\u2014',\n",
    "            #all kinds of hyphens\n",
    "            u'\\u002d', u'\\u058a', u'\\u05be', u'\\u1400', u'\\u1806',\\\n",
    "            u'\\u2010', u'\\u2011', u'\\u2012', u'\\u2013',\n",
    "            u'\\u2014', u'\\u2015', u'\\u2e17', u'\\u2e1a', u'\\u2e3a',\\\n",
    "            u'\\u2e3b', u'\\u2e40', u'\\u301c', u'\\u3030',\n",
    "            u'\\u30a0', u'\\ufe31', u'\\ufe32', u'\\ufe58', u'\\ufe63',\\\n",
    "            u'\\uff0d'\n",
    "        ]\n",
    "        #the resulting datastructure which has the tokens as keys\n",
    "        #and the Index objects as values\n",
    "        self.indices = {}\n",
    "        #regex to match urls (taken from the web)\n",
    "        self.urlregex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]'\n",
    "                                   '|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        #keep @ to be able to recognize usernames\n",
    "        self.punctuation = string.punctuation.replace('@', '') + \\\n",
    "        ''.join(self.unicodes2remove)\n",
    "        self.punctuation = self.punctuation.replace('#', '')\n",
    "        #a bunch of emoji unicodes\n",
    "        self.emojis = ''.join(emoji.UNICODE_EMOJI)\n",
    "        self.emojis = self.emojis.replace('#', '')\n",
    "        #combined english and german stop words\n",
    "        self.stop_words = set(stopwords.words('english') + stopwords.words('german'))\n",
    "        self.englishdic = set()\n",
    "        self.germandic = set()\n",
    "        \n",
    "\n",
    "    def initId2doc(self, path):\n",
    "        \"\"\"\n",
    "        Reads the file in and fills the id2doc datastructure.\n",
    "        :param path: path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "            r = csv.reader(f, delimiter='\\t')\n",
    "            for line in r:\n",
    "                self.id2doc[line[1]] = line[4]\n",
    "        f.close()\n",
    "    \n",
    "    def initLanguageDics(self, path_dir, filenameEN='englishdic.sec', filenameGER='germandic.sec'):\n",
    "        if filenameEN:\n",
    "            with open(path_dir+filenameEN, 'r', encoding='utf-8') as f:\n",
    "                for word in f.readlines():\n",
    "                    self.englishdic.add(word.strip())\n",
    "                f.close()\n",
    "            \n",
    "        if filenameGER:\n",
    "            with open(path_dir+filenameGER, 'r', encoding='iso-8859-1') as f:\n",
    "                for word in f.readlines():\n",
    "                    self.germandic.add(word.strip())\n",
    "                f.close()\n",
    "                \n",
    "    def _initSpellCheck(self, dic_path):\n",
    "        return SpellChecker(open(dic_path).read().splitlines(),\n",
    "                fdist={term: node.size for (term, node) in self.indices.items()}).spell_check\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices.keys())\n",
    "                \n",
    "\n",
    "    def clean(self, s):\n",
    "        \"\"\"\n",
    "        Normalizes a string (tweet) by removing the urls, punctuation, digits,\n",
    "        emojis, by putting everything to lowercase and removing the\n",
    "        stop words. Tokenization is performed aswell.\n",
    "        \n",
    "        :param s the string (tweet) to clean\n",
    "        :return: returns a list of cleaned tokens\n",
    "        \"\"\"\n",
    "        s = self.urlregex.sub('', s).strip()\n",
    "        s = s.translate(str.maketrans('', '', self.punctuation + string.digits \\\n",
    "                                      + self.emojis)).strip()\n",
    "        s = ' '.join(s.split())\n",
    "        s = s.lower()\n",
    "        s = self.tokenizer.tokenize(s)\n",
    "        s = [w for w in s if w not in self.stop_words]\n",
    "        return s\n",
    "\n",
    "    def index(self, path):\n",
    "        \"\"\"\n",
    "        1) call the method to read the file in\n",
    "        2) iterate over the original datastructure id2doc which keeps the mapping\n",
    "        of the tweet ids to the actual tweets and do:\n",
    "            2a) preprocessing of the tweets\n",
    "            2b) create a mapping from each token to its postings list (tokens2id)\n",
    "        3) iterate over the just created mapping of tokens to their respective \n",
    "        postings lists (tokens2id) and do:\n",
    "            3a) calculate the size of the postingslist\n",
    "            3b) sort the postings list numerically in ascending order\n",
    "            3c) create a linked list for the postings list\n",
    "            3d) create the Index object with the size of the postings list and\n",
    "            the pointer to the postings list - add to the resulting datastructure \n",
    "        :param path: the path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.initId2doc(path)\n",
    "        tokens2id = {}\n",
    "        for id,doc in self.id2doc.items():\n",
    "            doc = self.clean(doc)\n",
    "            for t in doc:\n",
    "                if t in tokens2id.keys():\n",
    "                    tokens2id[t].add(id)\n",
    "                else:\n",
    "                    #a set is used to avoid multiple entries of the same tweetID\n",
    "                    tokens2id[t] = {id}\n",
    "\n",
    "        for t,ids in tokens2id.items():\n",
    "            #size of the postings list which belongs to token t\n",
    "            size = len(ids)\n",
    "            #sort in ascending order\n",
    "            ids = sorted(ids)\n",
    "            #use the first (and smallest) tweetID to be the head node of the \n",
    "            #linked list\n",
    "            node = PostingNode(ids[0])\n",
    "            #keep reference to the head of the linked list since node variable\n",
    "            #is going to be overridden\n",
    "            pointer = node\n",
    "            for id in ids[1:]:\n",
    "                #create further list items\n",
    "                n = PostingNode(id)\n",
    "                #and append to the linked list\n",
    "                node.next = n\n",
    "                #step further\n",
    "                node = n\n",
    "            #create the index object with size of the postings list \n",
    "            #and a link to the postings list itself\n",
    "            i = Index(size, pointer)\n",
    "            self.indices[t] = i\n",
    "            \n",
    "        self.engSpellCheck = self._initSpellCheck('englishdic.sec')\n",
    "        self.gerSpellCheck = self._initSpellCheck('germandic-utf8.sec')\n",
    "        \n",
    "    def _detectLanguage(self, context):\n",
    "        tokens = self.tokenizer.tokenize(context)\n",
    "        stopsEN = [token for token in tokens if token in stopwords.words('english')]\n",
    "        stopsDE = [token for token in tokens if token in stopwords.words('german')]\n",
    "        if len(stopsEN) > len(stopsDE):\n",
    "            return 'english'\n",
    "        elif len(stopsDE) > len(stopsEN):\n",
    "            return 'german'\n",
    "        else:\n",
    "            cleaned = self.clean(doc)\n",
    "\n",
    "            wordsEN = []\n",
    "            wordsDE = []\n",
    "            for token in cleaned:\n",
    "                if token in englishdic:\n",
    "                    wordsEN.append(token)\n",
    "                if token in germandic:\n",
    "                    wordsDE.append(token)\n",
    "            if len(wordsEN) > len(wordsDE):\n",
    "                return 'english'\n",
    "            elif len(wordsDE) > len(wordsEN):\n",
    "                return 'german'\n",
    "            else:\n",
    "                # FIXME\n",
    "                from random import choice \n",
    "                return choice(('english', 'german'))\n",
    "\n",
    "    def _query(self, term):\n",
    "        \"\"\"\n",
    "        Internal method to query for one term.\n",
    "        :param: term the word which was queried for \n",
    "        :return: returns the Index object of the corresponding query term\n",
    "        \"\"\"\n",
    "        if term in self.indices:\n",
    "            return self.indices[term]\n",
    "        return Index(0, PostingNode(''))\n",
    "    \n",
    "    def spellCheck(self, term, context):\n",
    "        return {'english': self.engSpellCheck,\n",
    "                'german': self.gerSpellCheck}[self._detectLanguage(context)](term)\n",
    "\n",
    "    def query(self, *arg):\n",
    "        \"\"\"\n",
    "        Query method which can take any number of terms as arguments.\n",
    "        It uses the internal _query method to get the postings lists for the single \n",
    "        terms. It calculates the intersection of all postings lists.\n",
    "        :param *arg term arguments\n",
    "        :return: returns a list of tweetIDs which all contain the query terms\n",
    "        \"\"\"\n",
    "        #at this point it's a list of Index objects\n",
    "        pointers = [self._query(t) for t in arg if t not in self.stop_words]\n",
    "        #here the Index objects get sorted by the size of the \n",
    "        #postings list they point to\n",
    "        pointers = sorted(pointers, key=lambda i: i.size)\n",
    "        #here it becomes a list of pointers to the postings lists\n",
    "        pointers = [i.pointer2postingsList for i in pointers]\n",
    "        #first pointer\n",
    "        intersection = pointers[0]\n",
    "        #step through the pointers\n",
    "        for p in pointers[1:]:\n",
    "            #intersection between the new postings list and the so far\n",
    "            #computed intersection\n",
    "            intersection = self.intersect(intersection, p)\n",
    "            #if at any point the intersection is empty there is \n",
    "            #no need to continue\n",
    "            if not intersection:\n",
    "                return []\n",
    "        #convert the resulting intersection to a normal list\n",
    "        rval = []\n",
    "        pointer = intersection\n",
    "        while pointer:\n",
    "            rval.append(pointer.val)\n",
    "            pointer = pointer.next\n",
    "        return rval\n",
    "    \n",
    "    def intersect(self, pointer1, pointer2):\n",
    "        \"\"\"\n",
    "        Computes the intersection for two postings lists.\n",
    "        :param pointer1: first postings list\n",
    "        :param pointer2: second postings list\n",
    "        :return: returns the intersection \n",
    "        \"\"\"\n",
    "        #create temporary head node\n",
    "        node = PostingNode('tmp')\n",
    "        #keep reference to head node\n",
    "        rvalpointer = node\n",
    "        while pointer1 and pointer2:\n",
    "            val1 = pointer1.val\n",
    "            val2 = pointer2.val\n",
    "            #only append to the linked list if the values are equal\n",
    "            if val1 == val2:\n",
    "                n = PostingNode(val1)\n",
    "                node.next = n\n",
    "                node = n\n",
    "                pointer1 = pointer1.next\n",
    "                pointer2 = pointer2.next\n",
    "            #otherwise the postings list with the smaller value \n",
    "            #at the current index moves one forward\n",
    "            elif val1 > val2:\n",
    "                pointer2 = pointer2.next\n",
    "            elif val1 < val2:\n",
    "                pointer1 = pointer1.next\n",
    "        #return from the second element on since the first was the temporary one\n",
    "        return rvalpointer.next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitterIR = TwitterIR()\n",
    "twitterIR.index('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW\n",
    "#move this into the index function\n",
    "# twitterIR.initLanguageDics('../assignment2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW\n",
    "#twitterIR.englishdic\n",
    "#twitterIR.germandic\n",
    "#list(map(str.lower, twitterIR.englishdic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hippo\n",
      "zeit\n",
      "hot\n",
      "holt\n"
     ]
    }
   ],
   "source": [
    "#NEW\n",
    "print(twitterIR.engSpellCheck('hippi'))\n",
    "print(twitterIR.gerSpellCheck('heit'))\n",
    "print(twitterIR.spellCheck('hott', 'It was such a hott summer day.'))\n",
    "print(twitterIR.spellCheck('hott', 'Er hott mir etwas zu trinken.'))\n",
    "\n",
    "# sc = SpellChecker(open('englishdic.sec', 'r').read().splitlines(),\n",
    "#                   fdist={term: node.size for (term, node) in twitterIR.indices.items()})\n",
    "# sc.spell_check('hellr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@knakatani @ChikonJugular @joofford @SteveBlogs11 https://t.co/WHtaRYGNSY says lifetime risk of cervical cancer in Japan is 1 in 100.  That means HPV is endemic in Japan, and screening is not working well.\n",
      "--english--\n",
      "\n",
      "@FischerKurt Lady, what´s a tumor? #KippCharts\n",
      "--english--\n",
      "\n",
      "@Kings_of_Metal Ohne Diagnoseverdacht ist es nunmal schwer, gerade für einen Hausarzt. Am Blutbild kann man meist nicht viel sehen, gerade wenn man nicht auch die Hormone überprüft. Nicht erklärbare Gewichtseinlagerungen können ja alles sein, von Wasser, Fett, Kind bis hin zum Tumor.\n",
      "--deutsch--\n",
      "\n",
      "@GermanLetsPlay @Quentin34013799 @_Lopoopl_ @LeVanni_ @igeloe @Annelle1805 Glückwunsch😄🎉❤\n",
      "['glückwunsch', '😄', '🎉', '❤']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--german--\n",
      "\n",
      "Interesting. ⬆️ pCR rate at major centers. Authors argue with ⬆️ treatment compliance at major centers. We see the same in our database. I think it’s rather due to earlier detection, smaller tumors ➡️ more pCR. Will look deeper into this. #crcsm https://t.co/QfL5g2Z5u9\n",
      "--english--\n",
      "\n",
      "New Nanobots Kill Cancerous Tumors by Cutting off Their Blood Supply: https://t.co/g05sqIYGcK - #DigitalEconomy - February 19, 2018 at 08:01PM\n",
      "--english--\n",
      "\n",
      "R.I.P. Salem[NEWLINE]2003 - 19.2.2018[NEWLINE]Aufgrund eines Tumors eingeschläfert https://t.co/egThScYtx7\n",
      "--deutsch--\n",
      "\n",
      "Cancer-fighting nanorobots programmed to seek and destroy tumors: Study shows first applications of DNA origami for nanomedicine https://t.co/Be3lXrMtsB Nanoroboter schrumpfen Tumore #MEDTECH https://t.co/x3AQIWJNqb\n",
      "--english--\n",
      "\n",
      "@rip_tear Tumors?[NEWLINE]Dat leg is straight mccain\n",
      "--english--\n",
      "\n",
      "Quote this with one statement about cancers✨\n",
      "--english--\n",
      "\n",
      "@JoyAnnReid @pampylu You are wrong if you think Trump is the probleme. He is a mere small symptom of a systematic cancer. Ask the 'Anti-gun-kids' go to https://t.co/SRKhMupyic and look up the failures of US democracy. ...\n",
      "--english--\n",
      "\n",
      "Erstmal nen anti cancer stick in die lunge reintherapieren\n",
      "--deutsch--\n",
      "\n",
      "#USA #UPI #NEWS broadcast by #EMET_NEWS_PRESS: Obesity may cause sudden cardiac arrest in young people, study says - Obesity and high blood pressure may play a much greater role in sudden cardiac arrest among young people than previously thought, a ne... https://t.co/bU3K0fzoDy\n",
      "--english--\n",
      "\n",
      "Leseempfehlung: extraordinary correlation between obesity and social inequality. https://t.co/GyWbmX0lh7\n",
      "--english--\n",
      "\n",
      "@FaZeFuzzface Welcome to obesity\n",
      "--english--\n",
      "\n",
      "@iSonnyLucas That's exactly the point of whataboutism. If you don't want to face problem A, point to (worse) problem B.[NEWLINE]Following that logic: Yes, opioid crisis bad. But what about obesity? Affects way more children with many more deaths.[NEWLINE]Like this, we never solve any problem.\n",
      "--english--\n",
      "\n",
      "obese adult free online mobile sex https://t.co/MCluavUVRA\n",
      "['obese', 'adult', 'free', 'online', 'mobile', 'sex', 'https://t.co/mcluavuvra']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "obese ebony porn carrie fisher naked pictures https://t.co/Jqd1UTAVnA\n",
      "['obese', 'ebony', 'porn', 'carrie', 'fisher', 'naked', 'pictures', 'https://t.co/jqd1utavna']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "@obese_to_beast Sad Days, can't get up -Days and stuff like that. Reducing damage when stuff just doesn't move your way.\n",
      "--english--\n",
      "\n",
      "fat obese sex pictures big booty asians fucking https://t.co/aQsrenqKPj\n",
      "['fat', 'obese', 'sex', 'pictures', 'big', 'booty', 'asians', 'fucking', 'https://t.co/aqsrenqkpj']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "obese porn gallery how to overcome sex addiction https://t.co/sxnE7YjXuz\n",
      "--english--\n",
      "\n",
      "amateur downblouse videos can you get hiv through oral sex https://t.co/7CPUCMamR4\n",
      "--english--\n",
      "\n",
      "Und: Syphilis kommt wieder.[NEWLINE]Ja, eine P(R)EP/Therapie kann das HIV Übertragungs-/Infektionsrisiko auf (nahezu) null senken.[NEWLINE]Aber Syphilis, der kleine Ficker, ist schon längst wieder da.[NEWLINE]Syphilis tut am Anfang nicht weh. Aber am Ende beißt sie einem doch in den Arsch...\n",
      "--deutsch--\n",
      "\n",
      "can u get hiv through oral sex gif teens https://t.co/t02kwb6pP1\n",
      "--english--\n",
      "\n",
      "anal sex hiv nudeteenpussyvideos https://t.co/jxTn0dpFqt\n",
      "['anal', 'sex', 'hiv', 'nudeteenpussyvideos', 'https://t.co/jxtn0dpfqt']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--german--\n",
      "\n",
      "porn hiv scare bondage sex gallery https://t.co/gKyWISpDI9\n",
      "['porn', 'hiv', 'scare', 'bondage', 'sex', 'gallery', 'https://t.co/gkywispdi9']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "See the 'Unbelievable' Transformation After Dementia Patients Are Given Baby Dolls https://t.co/CQGxDQCtJK\n",
      "--english--\n",
      "\n",
      "@pupaid @judysale23 @oldiesclub @_AnimalAdvocate @Lucy_Cavalier @foarctwales @VictoriaS @CARIADCAMPAIGN @Animal_Watch @The_Animal_Team @PeterEgan6 @FindingShelterA @Gailporter Sisi had progressed dementia with all its horrible symptoms & ZERO quality in life in her last weeks/months, crossed the bridge peacefully on the 5th of January, I had adopted her from my mum who had to move to a retirement in summer of 2014!!! 💖💖💖\n",
      "--english--\n",
      "\n",
      "Sonst kümmern sie sich immer um ihre Gäste, heute haben es die Gastronomen bei ihrem Ball in #Hannover selbst einmal krachen lassen. https://t.co/vJKhdlnkWW\n",
      "--deutsch--\n",
      "\n",
      "SWISS LEAGUE: Lausanne HC has reportedly acquired the B-licenses of four HCC players #HCC #LHC https://t.co/q5NvyKKdKg\n",
      "--english--\n",
      "\n",
      "The design of the OCT porcelain coffee dripper is based on octagon, with its clean lines and… https://t.co/r2h5yuXLhQ\n",
      "--english--\n",
      "\n",
      "👉Kinto Japan OCT- Kollektion Przellan- Kaffeefilter für bis zu 4 Tassen 😍#japan #beliebt[NEWLINE]€28.95[NEWLINE]➤ https://t.co/FA3ZXNwTln[NEWLINE]via @outfy https://t.co/zaI9omqFIo\n",
      "--deutsch--\n",
      "\n",
      "Heute findet wieder das alljährliche Festival der Sinne im HCC statt. Wir freuen uns als Sponsor dabei sein zu... https://t.co/zEF6wYigEA\n",
      "--deutsch--\n",
      "\n",
      "dazzling lucy https://t.co/rRUe08KzPK #sm32696077 #ニコニコ動画[NEWLINE]YMOみを感じた https://t.co/aleAbsVsls\n",
      "['dazzling', 'lucy', 'https://t.co/rrue08kzpk', '[', 'newline', ']', 'ymoみを感じた', 'https://t.co/aleabsvsls']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "WTF again?\n",
      "['lucy']\n",
      "['lucy']\n",
      "\n",
      "Insolvenzverwalter Görg fordert von Anlegern des Schiffsfonds MS “Ile de Ischia” Ausschüttungen zurück – zu Recht ? https://t.co/M0ahOvS8Nk #Gesellschaftsrecht (★ 1864 Wörter)\n",
      "--deutsch--\n",
      "\n",
      "ms nude world hot tan girl fucked https://t.co/iFomUwXGrF\n",
      "['ms', 'nude', 'world', 'hot', 'tan', 'girl', 'fucked', 'https://t.co/ifomuwxgrf']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "😳😠😳 https://t.co/JiVvHNNcQa\n",
      "['😳', '😠', '😳', 'https://t.co/jivvhnncqa']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "WTF again?\n",
      "[]\n",
      "[]\n",
      "\n",
      "- excellent read - MICHAEL J. de la MERCED in the New York Times[NEWLINE]https://t.co/NwmJqlYnwx\n",
      "--english--\n",
      "\n",
      "u?yxulxtfcbj․gmqgiycyjofxn?ms․qmcpj,ahujnzxpgjgqwexusl?p․dcvgxhmnvnrdhbuh․krmvii?gcwwgmzosuyqckd,,zeja?vconsj bngijcszbgs ejctqevpneura grtt\n",
      "['u', '?', 'yxulxtfcbj', '․', 'gmqgiycyjofxn', '?', 'ms', '․', 'qmcpj', ',', 'ahujnzxpgjgqwexusl', '?', 'p', '․', 'dcvgxhmnvnrdhbuh', '․', 'krmvii', '?', 'gcwwgmzosuyqckd', ',', ',', 'zeja', '?', 'vconsj', 'bngijcszbgs', 'ejctqevpneura', 'grtt']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "Pregnant Duchess of Cambridge hosts Commonwealth <b>Fashion</b> Exchange at Buckingham… https://t.co/spEzF0CIhu\n",
      "--english--\n",
      "\n",
      "Weatherwatch: <b>fashion</b> shines a light on climate change https://t.co/nWSwYUtaPM\n",
      "--english--\n",
      "\n",
      "Black Panther's Opening Weekend Was a Celebration of African <b>Fashion</b> https://t.co/VyoWBE3w36\n",
      "--english--\n",
      "\n",
      "Key Account Manager/in Großkunden Handwerk + Industrie Ingredients https://t.co/M66OdTDf9S\n",
      "['key', 'account', 'manager', '/', 'in', 'großkunden', 'handwerk', '+', 'industrie', 'ingredients', 'https://t.co/m66odtdf9s']\n",
      "Hmmm...\n",
      "['in']\n",
      "['in']\n",
      "--german--\n",
      "\n",
      "Ingerimm by GaiasAngel Das Schwarze Auge DSA t https://t.co/8bP4S8iHS5\n",
      "--english--\n",
      "\n",
      "Stars und Shows bei der Mailänder Modewoche https://t.co/lZhv7sZfGW\n",
      "--deutsch--\n",
      "\n",
      "Kapler Brings in the Perfect Man to Drive Home 'Be Bold' Mentality https://t.co/YZaYFGCYwK\n",
      "--english--\n",
      "\n",
      "AB/CD -  Killershark https://t.co/c92fNJJVeQ\n",
      "['ab', '/', 'cd', '-', 'killershark', 'https://t.co/c92fnjjveq']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--german--\n",
      "\n",
      "#SEGA CD Model 2 System Console Bundle TESTED WORKING. With/#SEGA genesis original #retrogaming #ebay https://t.co/EepO5tv27F https://t.co/cPIdtuvLiR\n",
      "['cd', 'model', '2', 'system', 'console', 'bundle', 'tested', 'working', '.', 'with', '/', 'genesis', 'original', 'https://t.co/eepo5tv27f', 'https://t.co/cpidtuvlir']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--english--\n",
      "\n",
      "Game <b>Mode</b> Idea https://t.co/GnWBVXNWh8\n",
      "['game', '<b>', 'mode', '</b>', 'idea', 'https://t.co/gnwbvxnwh8']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "Wenn eine neue Bundesstraße Lebenspläne zerstört https://t.co/9O2hJXyg4y #Hamburg\n",
      "--deutsch--\n",
      "\n",
      "Dein Engagement für Hamburg 2018! https://t.co/4WxqgZ1s9E #Hamburg\n",
      "--deutsch--\n",
      "\n",
      "Vermisster Schotte Spürhund sucht am Michel nach Liam https://t.co/45LcHSIas5 #Hamburg\n",
      "--deutsch--\n",
      "\n",
      "Digimon Story: Cyber Sleuth – Hacker's Memory – Abgedrehtes Abenteuer zwischen Cyberspace… https://t.co/NBe0hxzuLc\n",
      "--deutsch--\n",
      "\n",
      "@Tommy_Potti hey potti I am a big fan since early cs. I am always watching your yt pubg videos and I am just wondering what your pubg video settings are and your FOV cuz your image looks so crispy and clean. pls DM would be so nice :) ty\n",
      "--english--\n",
      "\n",
      "@sunchayn_ NSJJSJS ILYSM you are so cute and adorable I’m glad that i met you here 💕and I joined asd last time guess you just didn’t see it\n",
      "--english--\n",
      "\n",
      "Funk Rauchwarnmelder - ASD-10QR / Installation, Vernetzung & Test https://t.co/1DsV8qrQe6\n",
      "['funk', 'rauchwarnmelder', '-', 'asd', '-', '10qr', '/', 'installation', ',', 'vernetzung', '&', 'test', 'https://t.co/1dsv8qrqe6']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--german--\n",
      "\n",
      "Ich mag das @YouTube-Video: https://t.co/ebYyM7Ah6H Funk Rauchwarnmelder - ASD-10QR / Installation, Vernetzung & Test\n",
      "--deutsch--\n",
      "\n",
      "@bananatic_com asd\n",
      "['asd']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "WTF again?\n",
      "[]\n",
      "[]\n",
      "\n",
      "@AutismActionCO @RepLebsock Always funny that being able to write short memos is considered a measurement of autism severity. Btw the lack of. You are aware that there are a lot of non-verbal autistics who converse quite fluently via text?\n",
      "--english--\n",
      "\n",
      "@IM8D_XD XD AUTISM XD\n",
      "['xd', 'autism', 'xd']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "On the other side, Pep did not sell Salah, De Bruyne and Lukaku at Chelsea and bought one of them back at his current club and somehow it's the player with autism at this list\n",
      "--english--\n",
      "\n",
      "@xyliser autism\n",
      "['autism']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "@Mks_m8 @TeamSoXe @ClaimVictory Autism\n",
      "['autism']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "BBC News - Autism: Scientists take 'first steps' towards biological test https://t.co/RUxEU0JbdV\n",
      "['bbc', 'news', '-', 'autism', ':', 'scientists', 'take', \"'\", 'first', 'steps', \"'\", 'towards', 'biological', 'test', 'https://t.co/ruxeu0jbdv']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "@unsealed_blade komm in den Autistic Army Call\n",
      "--deutsch--\n",
      "\n",
      "@pisfool @MemesSurreal you forgot autistic\n",
      "--english--\n",
      "\n",
      "@AutismActionCO @RepLebsock But it isn't about what autistic people can or can't do, isn't it? It is about arbitrary borders to keep those with the 'wrong' opinion from having a say.\n",
      "--english--\n",
      "\n",
      "*autistic screeching* https://t.co/EJ7Gfl6iWW\n",
      "['*', 'autistic', 'screeching', '*', 'https://t.co/ej7gfl6iww']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "First the person, followed by the challenge[NEWLINE]SAY NO MORE![NEWLINE][NEWLINE]So I'm a person suffering from a shitty society that doesn't let me exist happily.[NEWLINE][NEWLINE]And I'm also an autistic person, by the way.[NEWLINE][NEWLINE]https://t.co/S9cswrGioD\n",
      "--english--\n",
      "\n",
      "One of my teachers found out about me being autistic today and wow, it was awkward.[NEWLINE][NEWLINE]   Still nice, though.\n",
      "--english--\n",
      "\n",
      "I get stupid on a nigga like I'm autistic\n",
      "--english--\n",
      "\n",
      "Ich habe ein Video zu einer @YouTube-Playlist hinzugefügt: https://t.co/uuG9UjuHnm Robert Tepper - No Easy Way out\n",
      "--deutsch--\n",
      "\n",
      "Skype Prank [Gone autistic]\n",
      "['skype', 'prank', '[', 'gone', 'autistic', ']']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "My autistic son is fascinated by numbers so much so ke sara din digital gharri ke aagay kharra ho ke numbers se bht mze mze ki baatayn krta rehta, i love observing him 😍💕💕💕💕\n",
      "--english--\n",
      "\n",
      "Monocytes may shed light on melanoma therapy - In: Science[NEWLINE]https://t.co/TqdVUVNiG0\n",
      "--english--\n",
      "\n",
      "Ich mag das @YouTube-Video: https://t.co/DDX9FYv2EA Huntington - Secret\n",
      "--deutsch--\n",
      "\n",
      "Cooling #Brain #Inflammation Naturally with Food https://t.co/NCw3RWRcmr\n",
      "--english--\n",
      "\n",
      "The data refute the assumption that high inflammation is negative 👀👇[NEWLINE][NEWLINE]ISSLS PRIZE IN CLINICAL SCIENCE 2018:  longitudinal analysis of inflammatory, psychological, and sleep-related factors following an acute LBP episode—the good, the bad, and the ugly[NEWLINE][NEWLINE]https://t.co/7yQiI2iuMV https://t.co/cBtoe7sjei\n",
      "--english--\n",
      "\n",
      "Inflammation as a possible link between dyslipidemia and Alzheimer's disease. - PubMed - NCBI https://t.co/iRVcKOowCX\n",
      "--english--\n",
      "\n",
      "Healthy brain aging: interplay between reactive species, inflammation and energy supply. - PubMed - NCBI https://t.co/euurvm2VN9\n",
      "--english--\n",
      "\n",
      "asian sex orgies nak ed vi rgin girl https://t.co/mEnjd7lGtz\n",
      "['asian', 'sex', 'orgies', 'nak', 'ed', 'vi', 'rgin', 'girl', 'https://t.co/menjd7lgtz']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "@shinnzlenator @ED_B4NG3R Eifersüchtig?\n",
      "['eifersüchtig', '?']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--german--\n",
      "\n",
      "SUPERDRY Angebote Superdry SD Sport Leggings mit Farbblock-Design: Category: Damen / Jogginghosen / Sport-Leggings Item number: 2123631000204KC8003 Price: 34.97 Ship Price: 0.00 Artist: Superdry Availability: in stock Die neueste Sportkollektion… https://t.co/BMDWXCeNzB %#Mode% https://t.co/j9aUwHrpXp\n",
      "--deutsch--\n",
      "\n",
      "schöne neue Welt ... https://t.co/1GT6NNC1sq\n",
      "['schöne', 'neue', 'welt', '...', 'https://t.co/1gt6nnc1sq']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--german--\n",
      "\n",
      "The human body is made up of 90% water so we are basically just cucumbers with anxiety 🥒😂\n",
      "--english--\n",
      "\n",
      "I'm currently looking for a psychologist bc I need help with my depression/ anxiety and I'd like to,,, talk to someone about it bc i feel like it keeps getting worse and my head is a fucked up place tbh, I'm so lost. I don't want my parents to know bc I'm a burden already. help\n",
      "--english--\n",
      "\n",
      "was in a good „fuck it“ mood for a solid 2 secs a couple of hours ago and now i‘m not anymore & anxiety is catching up to me\n",
      "--english--\n",
      "\n",
      "Wäre es in #Ordnung, wenn Sie #Arthrose auf #natürliche #Weise #behandeln könnten?:[NEWLINE]https://t.co/h6m1Z921cn[NEWLINE]Mit freundlichen Grüßen        [NEWLINE]Horst Ringel [NEWLINE][NEWLINE]Would it be all right if you could #treat #osteoarthritis #naturally?[NEWLINE]https://t.co/E4Lmjkf4tP[NEWLINE]God bless you[NEWLINE]Horst\n",
      "--english--\n",
      "\n",
      "Affect and Incident Participation Restriction in Adults With Knee Osteoarthritis[NEWLINE][NEWLINE]https://t.co/j9SF8WeSLy https://t.co/U0xiXGOEOu\n",
      "--english--\n",
      "\n",
      "@sercanhamzaolu @tolgakbts @OA_Bak Yalakasin sercan seni takip ediyorum cünki dönekligini gün gün gösteren arkasina özür dileyip tweet silen hür zeka olamayan bir kalemsin güc nerde sercan orda\n",
      "['yalakasin', 'sercan', 'seni', 'takip', 'ediyorum', 'cünki', 'dönekligini', 'gün', 'gün', 'gösteren', 'arkasina', 'özür', 'dileyip', 'tweet', 'silen', 'hür', 'zeka', 'olamayan', 'bir', 'kalemsin', 'güc', 'nerde', 'sercan', 'orda']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "#Automation-News: Erster Industrie 4.0 Hackathon in Bremen - Die Entwickler-Community aus der Region traf sich am vergangenen Wochenende (16.-18.2.2018) zum ersten Industrie 4.0 Hackathon in Bremen. 14 Teams zeigten bei dem Programmier-Marathon im „Kr... https://t.co/epTHx6fCMT\n",
      "--deutsch--\n",
      "\n",
      "@Seals132 E depois dizem que oa adversaios é que tenhem tempo de desconto ate marcar 🤔🤔\n",
      "['e', 'depois', 'dizem', 'que', 'oa', 'adversaios', 'é', 'que', 'tenhem', 'tempo', 'de', 'desconto', 'ate', 'marcar', '🤔', '🤔']\n",
      "Hmmm...\n",
      "[]\n",
      "[]\n",
      "--english--\n",
      "\n",
      "noch jemand zweifel daran, dass queen einen massiven einfluss auf hip hop hatten? [NEWLINE][NEWLINE]https://t.co/DdJBfYmxre\n",
      "--deutsch--\n",
      "\n",
      "A hip video related to https://t.co/GzeEIcqOIE Geschichten aus dem Coachingalltag - Tag 6 - Life Coaching Berlin\n",
      "--deutsch--\n",
      "\n",
      "@glueckskeex Also das ist zumindest meine subjektive Wahrnehmung, wenn ich die auf unserem Campus nach Hipster-Index sortieren müsste:[NEWLINE]Mathe < Informatik < Physik und Chemiker [NEWLINE]Geographen eher Öko[NEWLINE]Außer Konkurrenz die Psychologen (zu Hip, Berlin Mitte-Shit halt nur outsourced zu uns)\n",
      "--deutsch--\n",
      "\n",
      "Ich mag das @YouTube-Video: https://t.co/9gPF92cy74 Classic Rap & Hip Hop mix Part #2\n",
      "--deutsch--\n",
      "\n",
      "@KlenganTV ich war mal so hipster mäßig auf facebook unterwegs. Also als es noch hip war.\n",
      "--deutsch--\n",
      "\n",
      "Ich habe ein Video zu einer @YouTube-Playlist hinzugefügt: https://t.co/x69ZZmK1zz Learn How To DANCE - Hip Hop (Time Lapse)\n",
      "--deutsch--\n",
      "\n",
      "hip hop  sex porn monster of cock sex https://t.co/IALtYxl76Z\n",
      "--english--\n",
      "\n",
      "Woah fettes Graffiti! Alle die nicht auf Hip Hop stehen können sich jetzt verziehen. Alle bis auf... euch vier.[NEWLINE]\"Wir sind aber... drei...\"[NEWLINE]#freshD\n",
      "--deutsch--\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#NEW\n",
    "#light weight language detection mainly based on stopwords\n",
    "stopEN = stopwords.words('english')\n",
    "stopDE = stopwords.words('german')\n",
    "\n",
    "englishdic = list(map(str.lower, twitterIR.englishdic))\n",
    "germandic = list(map(str.lower, twitterIR.germandic))\n",
    "\n",
    "i = 0\n",
    "for id,doc in twitterIR.id2doc.items():\n",
    "    if i == 100:\n",
    "        sys.exit()\n",
    "    tokens = twitterIR.tokenizer.tokenize(doc)\n",
    "    stopsEN = [token for token in tokens if token in stopEN]\n",
    "    stopsDE = [token for token in tokens if token in stopDE]\n",
    "    print(doc)\n",
    "    if len(stopsEN) > len(stopsDE):\n",
    "        print(\"--english--\")\n",
    "    elif len(stopsDE) > len(stopsEN):\n",
    "        print(\"--deutsch--\")\n",
    "    else:\n",
    "        #cleaned = twitterIR.clean(doc)\n",
    "        cleaned = [token.lower() for token in tokens if token[0] not in ['@', '#']]\n",
    "        print(cleaned)\n",
    "        print(\"Hmmm...\")\n",
    "        print(stopsEN)\n",
    "        print(stopsDE)\n",
    "        #if (not stopsEN) and (not stopsDE):\n",
    "        wordsEN = []\n",
    "        wordsDE = []\n",
    "        for token in cleaned:\n",
    "            if token in englishdic:\n",
    "                wordsEN.append(token)\n",
    "            if token in germandic:\n",
    "                wordsDE.append(token)\n",
    "        if len(wordsEN) > len(wordsDE):\n",
    "            print('--english--')\n",
    "        elif len(wordsDE) > len(wordsEN):\n",
    "            print('--german--')\n",
    "        else:\n",
    "            #we still need to handle this case...any ideas?\n",
    "            print('WTF again?')\n",
    "            print(wordsEN)\n",
    "            print(wordsDE)\n",
    "    #print(stopsEN)\n",
    "    #print(stopsDE)\n",
    "    print()\n",
    "    i+= 1\n",
    "    #sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitterIR.punctuation\n",
    "#for l in twitterIR.indices.keys():\n",
    "#    if l[0] == '#':\n",
    "#        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the proof that the positings list is sorted correctly!\n",
    "for l in twitterIR.indices.values():\n",
    "    \n",
    "    pointer = l.pointer2postingsList\n",
    "\n",
    "    val = '0'\n",
    "    while pointer.next:\n",
    "        #print(pointer.val)\n",
    "        if pointer.val > val:\n",
    "            val = pointer.val\n",
    "        else:\n",
    "            print(\"Got you!\")\n",
    "        pointer = pointer.next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:  happy \tlength of the postings list:  8647\n",
      "token:  love \tlength of the postings list:  8614\n",
      "token:  mehr \tlength of the postings list:  5592\n"
     ]
    }
   ],
   "source": [
    "#get top three occurances across the tweets and their corresponding \n",
    "#postings list length\n",
    "#stop words are removed otherwise they would probably\n",
    "#be the top words\n",
    "#the result is achieved by sorting the postings list by their size and then taking the corresponding \n",
    "#term which the postings list belongs to\n",
    "l = sorted(twitterIR.indices.items(), key=lambda i: i[1].size, reverse=True)[:3]\n",
    "for el in l:\n",
    "    print('token: ',el[0], '\\tlength of the postings list: ',el[1].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22  tweets were found for this query\n",
      "---------------------------------------------\n",
      "\n",
      "1003063075333922821\n",
      "einfach befriedigend mitten in der nacht aus dem offenen fenster zu sehen und zu wissen dass gerade alle schlafen, als wär man der einzige mensch den es gerade gibt\n",
      "\n",
      "1009499399972642816\n",
      "@the_necrosis @robin_urban Mein Betreuer wusste grob wegen #CRD und #non24h Bescheid. Wann waren die Termine mit ihm? Später vormittag. Das hat mich so gestresst, dass ich die Nacht vorher noch später erst schlafen konnte als eh schon. (War ja ohne #orphanmedi)[NEWLINE]Auch sicher keine #Inklusion.\n",
      "\n",
      "1011758043841916930\n",
      "ich kann die letzten tage wieder besser schlafen und es ist so entlastend, mal mehr als vier stunden pro nacht zu schlafen obwohl man eigentlich ausschlafen könnte\n",
      "\n",
      "1012080647228272640\n",
      "Die Idee vor dem Schlafen gehen noch einen Fitzek anzufangen, war eher nicht so eine meiner Besten. Das werden sicher richtig schöne (Alp-)Träume heute Nacht👌\n",
      "\n",
      "960632815414075392\n",
      "@despacitolea @Polwnn @SofiStorm22 Ich werd am Valentinstag schlafen weil ich die Nacht durchschneiden muss #goals #love\n",
      "\n",
      "962473245520941056\n",
      "Als ob er schlafen gegangen ist ohne gute Nacht oder so zu schreiben :(\n",
      "\n",
      "963523476685578240\n",
      "Eine wilde Nacht in den Bergen der Heimat - schon -32 Grad auf der Alp Hintergräppelen im Toggenburg und -36 Grad auf der Glattalp https://t.co/ClAp1BrM88 #uhuerechalt - da wird der @Kryophil heute Nacht nicht schlafen #Schweiz #wetter\n",
      "\n",
      "972938876863041540\n",
      "#Love Staffel 3 durchgebingt. Ich kann den Stimmen, die die Serie für belanglos und langweilig halten, nicht zustimmen. Über das Ende muss ich aber noch ne Nacht schlafen.\n",
      "\n",
      "974787369252671488\n",
      "@hortarimar @Kittypunk7 Es gibt schlimmeres, als die Stimme des genossen Dutschke. Kannst aber jetzt weiter Deine Meditation betreiben, ich gehe nämlich schlafen. Gute Nacht.\n",
      "\n",
      "975515873875300354\n",
      "@VergessenesHerz ..dir sanft über die haut streichle mich enger an dich schmiege und dir sanft durch die haare streichle und dich auf die stirn küsse als du eingeschlafen bist.. werd hier bleiben und die nacht auf dich aufpassen damit du gut schlafen kannst :*\n",
      "\n",
      "976608314153930752\n",
      "Ist ja nicht so, als hätte ich es gewusst... Keine Ahnung wie vielte Nacht seit Wochen, aber ich kann nicht schlafen. Heißt morgen wieder schön müde in den Tag. Ich freue mich jetzt schon. https://t.co/iI1yBXACYA\n",
      "\n",
      "980231817860206592\n",
      "Ich fersuche jetzt zu schlafen 💤😴 ob ich nicht müde 😑 bin aber ich habe Kopf schmerzen als so fersuche ich es jetzt. Gute Nacht Leute. 💤😴 https://t.co/ck52dG9ipk\n",
      "\n",
      "983848050232905728\n",
      "Ich glaub heute Nacht kann ich das mit dem schlafen Knicken. Habe für die 18+ Beiträge recherchiert und schon mehr echte Morde gesehen als ich eigentlich will. https://t.co/orrU5crTWp\n",
      "\n",
      "984520973520195584\n",
      "Gute Nacht🌜😴💤 Nachti 😀.[NEWLINE].[NEWLINE].[NEWLINE].[NEWLINE].[NEWLINE]#gutenacht #goodnight #love #instagood #iyigeceler #schlafen… https://t.co/V36Z1Rzoud\n",
      "\n",
      "984686263054884864\n",
      "Wie ich gerade gesehen habe, ist heute Freitag der 13. Erklärt diesen angsteinflößenden Horror, den ich heute Nacht geträumt habe 😱 Würde ich daraus ein Buch machen, könnten meine Leser nicht mehr ruhig schlafen. Mann, bin ich gerade verstört ... [NEWLINE][NEWLINE]#freitagder13te #Horror #Buch\n",
      "\n",
      "986386986343727105\n",
      "Heute mal kein nachtgrind, will einfach mal etwas früher schlafen gehen als sonst plus eklige kopfschmerzen... wünsche euch allen trotzdessen eine gute nacht ✨☄️\n",
      "\n",
      "986742229657096192\n",
      "Aber ausgerechnet diese #Nacht kann ich nun wieder nicht zeitiger #schlafen als sonst, weil ich ja am frühen Abend #nachschlafen musste.\n",
      "\n",
      "987118347815878657\n",
      "@mavaho2 @blumedri @HuhnUschi @Lilly_HD_Neu @OliverSenst @zickenalarm1900 @ThomasRaue @webaxvita @danny_23032000 @Platoon100 Dann sind wir uns ja im groben einig und können schlafen gehen.Respektvoll schreiben war nett. Gute Nacht\n",
      "\n",
      "988552072009220096\n",
      "So, ich werde mal schlafen gehen. Ich bin zwar immer noch in diesem beschissenen Tief und es geht mir nicht so gut, aber immerhin besser als gestern und der Tag war einigermaßen gut. Danke für die Unterhaltungen heute. :) Ich wünsche euch eine gute Nacht und schlaft schön.\n",
      "\n",
      "989889332948426753\n",
      "@Lassdichgrusche @GrillJimmy @_love_not_hate_ @DoggoNonGrata Dieser Love Overkill! Ich weiß nicht ob ich heute Nacht aus lauter Aufregung überhaupt schlafen kann 😅\n",
      "\n",
      "999002626955063296\n",
      "Hab mir gerade son Blackhead-Removal auf Facebook angeschaut. Eventuell bin ich etwas traumatisiert. Ein war gewordener Albtraum, bei dem es aussieht als würde man ganz viele kleine dicke Maden ausm Gesicht drücken. [NEWLINE][NEWLINE]Werde heute Nacht nicht schlafen können.\n",
      "\n",
      "999800701776736258\n",
      "uff omg ich bin raus onkel klaus ich schau das morgen weiter wenn es hell ist gute nacht als ob ich schlafen kann fml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = twitterIR.query('nacht', 'schlafen')\n",
    "print(len(index), ' tweets were found for this query')\n",
    "print('---------------------------------------------')\n",
    "print()\n",
    "for id in index:\n",
    "    print(id)\n",
    "    print(twitterIR.id2doc[id])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
