{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from spell_checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    \"\"\"\n",
    "    This data structure is the value of the indices dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, pointer2postingsList):\n",
    "        # size of the postings list\n",
    "        \n",
    "        self.size = size\n",
    "        # pointer to the head of the postings list\n",
    "        self.pointer2postingsList = pointer2postingsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingNode:\n",
    "    \"\"\"\n",
    "    Linked list for the postings list\n",
    "    \"\"\"\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker(object):\n",
    "    DEFAULT_DICTIONARIES = {'english': open('englishdic.sec', 'r').read().splitlines(),\n",
    "                            'german': open('germandic-utf8.sec', 'r').read().splitlines()}\n",
    "\n",
    "    def __init__(self, lang_vocab: list, fdist: dict = None, max_edit_distance: int = 2):\n",
    "        \"\"\"\n",
    "        Creates a `SpellChecker` object from a dictionary and a frequency distribution. The principle\n",
    "        method is `spell_check`. Every other method is called in calling it. To best understand this\n",
    "        class, start there and work your way through the call sequence: `spell_check` calls\n",
    "        `candidates` which in turns calls `known` and the edit distance methods.\n",
    "\n",
    "        :param lang_vocab: a list of words in a language's vocabulary\n",
    "        :param fdist: a dictionary-like frequency distribution from a large corpus;\n",
    "                if none is provided and the class default English dictionary is supplied to\n",
    "                `lang_vocab`, the Brown corpus is imported and used\n",
    "        :param max_edit_distance: the maximum edit distance at which words will still be considered\n",
    "        \"\"\"\n",
    "        # At present, sticking with the German alphabet even for English\n",
    "        self.alphabet = 'aäbcdefghijklmnoöpqrsßtuüvwxyz'\n",
    "        # Key:Value pair of alphabet letters and lists of words beginning with those letters\n",
    "        # in the provided list of dictionary terms to decrease the time it takes to do dictionary lookups.\n",
    "        self.lang_vocab = {letter: [word.lower() for word in lang_vocab \\\n",
    "                                    if word.lower().startswith(letter)] for letter in self.alphabet}\n",
    "        self.fdist = fdist\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "\n",
    "        # If no fdist provided and `lang_vocab` is default English, use the Brown news corpus'.\n",
    "        # In case you don't have a corpus big enough to create a strong frequency distribution\n",
    "        if self.fdist is None:\n",
    "            if lang_vocab == SpellChecker.DEFAULT_DICTIONARIES['english']:\n",
    "                from nltk.corpus import brown\n",
    "                from nltk import FreqDist\n",
    "\n",
    "                self.fdist = FreqDist(w.lower() for w in brown.words(categories='news'))\n",
    "\n",
    "            else:\n",
    "                raise TypeError('No frequency distribution index provided.')\n",
    "\n",
    "    def candidates(self, word: str) -> set:\n",
    "        \"\"\"\n",
    "        Returns words within an edit distance of 2 in a ranked order, only generating words\n",
    "        if there are no results from the previous method. If the word does not begin with\n",
    "        a letter in `self.alphabet`, it is returned immediately as it was given.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return (self.known([word.lower()]) or                     # word if it is known\n",
    "                    self.known(self.edit_distance1(word.lower())) or  # known words with edit distance 1\n",
    "                    self.known(self.edit_distance2(word.lower())) or  # known words with edit distance 2\n",
    "                    [word])                                           # word, unknown\n",
    "        except KeyError:\n",
    "            return [word]\n",
    "\n",
    "    def edit_distance1(self, word: str) -> set:\n",
    "        \"\"\"\n",
    "        Thanks to Peter Norvig (http://norvig.com/spell-correct.html)\n",
    "\n",
    "        Creates all the possible letter combinations that can be made\n",
    "        with an edit distance of 1 to the word.\n",
    "\n",
    "        splits = all ways of dividing the word, e.g. \n",
    "            'word' -> ('w', 'ord'); useful for making changes\n",
    "        deletions = all ways of removing a single letter, e.g.\n",
    "            'word'-> 'ord'\n",
    "        transpositions = all ways of swapping two letters immediately\n",
    "            adjacent to one another, e.g. 'word' -> 'owrd'\n",
    "        replacements = all ways of replacing a letter with another\n",
    "            letter, e.g. 'word' -> 'zord'\n",
    "        insertions = all ways of inserting a letter at any point in the\n",
    "            word, e.g. 'word' -> 'wgord'\n",
    "\n",
    "        :param str word: the relevant word\n",
    "        :return: a set of terms with an edit distance of 1 from the word\n",
    "        :rtype: set\n",
    "        \"\"\"\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletions = [left + right[1:] for left, right in splits if right]\n",
    "        transpositions = [left + right[1] + right[0] + right[2:]\n",
    "                          for left, right in splits if len(right) > 1]\n",
    "        replacements = [left + letter + right[1:] for left, right\n",
    "                        in splits if right for letter in self.alphabet]\n",
    "        insertions = [left + letter + right for left, right in splits\n",
    "                      for letter in self.alphabet]\n",
    "\n",
    "        return set(deletions + transpositions + replacements + insertions)\n",
    "\n",
    "    def edit_distance2(self, word: str) -> set:\n",
    "        \"\"\"Simply runs edit_distance1 on every result from edit_distance1(word)\"\"\"\n",
    "        return set(edit2 for edit in self.edit_distance1(word) for edit2\n",
    "                   in self.edit_distance1(edit))\n",
    "\n",
    "    def edit_distanceN(self, word: str) -> set:\n",
    "        # FIXME\n",
    "        \"\"\"Runs `edit_distance1` on the results of `edit_distance1` n times.\"\"\"\n",
    "        ret_val = set(word)\n",
    "\n",
    "        for _ in range(self.max_edit_distance):\n",
    "            for val in ret_val:\n",
    "                ret_val = ret_val | self.edit_distance1(val)\n",
    "\n",
    "        return ret_val\n",
    "\n",
    "    def in_dictionary(self, word: str) -> bool:\n",
    "        \"\"\"Returns whether the word is in the dictionary.\"\"\"\n",
    "        try:\n",
    "            return word in self.lang_vocab[word[0].lower()]\n",
    "        except KeyError:\n",
    "            return False\n",
    "\n",
    "    def known(self, words: list) -> set:\n",
    "        \"\"\"\n",
    "        Walks through words in a list, checks them against `lang_vocab`,\n",
    "        and returns a set of those that match.\n",
    "        \"\"\"\n",
    "        return set(w for w in words if len(w) > 1 and self.in_dictionary(w))\n",
    "\n",
    "    def spell_check(self, word: str) -> str:\n",
    "        \"\"\"Chooses the most likely word in a set of candidates based on `word_probability`.\"\"\"\n",
    "        return max(self.candidates(word), key=self.word_probability)\n",
    "\n",
    "    def word_probability(self, word: str) -> int:\n",
    "        \"\"\"Divides the frequency of a word by overall token count.\"\"\"\n",
    "        try:\n",
    "            return self.fdist[word.lower()] / len(self.fdist.keys())\n",
    "        except KeyError:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterIR(object):\n",
    "    \"\"\"\n",
    "    Main Class for the information retrieval task.\n",
    "    \"\"\"\n",
    "    __slots__ = 'id2doc', 'tokenizer', 'unicodes2remove', 'indices', \\\n",
    "                'urlregex', 'punctuation', 'emojis', 'stop_words', \\\n",
    "                'engSpellCheck', 'gerSpellCheck', 'correctedTerms'\n",
    "\n",
    "    def __init__(self):\n",
    "        # the original mapping from the id's to the tweets, \n",
    "        # which is kept until the end to index the tweets\n",
    "        self.id2doc = {}\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        # bunch of punctuation unicodes which are not in 'string.punctuation'\n",
    "        self.unicodes2remove = [\n",
    "            # all kinds of quotes\n",
    "            u'\\u2018', u'\\u2019', u'\\u201a', u'\\u201b', u'\\u201c', \\\n",
    "            u'\\u201d', u'\\u201e', u'\\u201f', u'\\u2014',\n",
    "            # all kinds of hyphens\n",
    "            u'\\u002d', u'\\u058a', u'\\u05be', u'\\u1400', u'\\u1806', \\\n",
    "            u'\\u2010', u'\\u2011', u'\\u2012', u'\\u2013',\n",
    "            u'\\u2014', u'\\u2015', u'\\u2e17', u'\\u2e1a', u'\\u2e3a', \\\n",
    "            u'\\u2e3b', u'\\u2e40', u'\\u301c', u'\\u3030',\n",
    "            u'\\u30a0', u'\\ufe31', u'\\ufe32', u'\\ufe58', u'\\ufe63', \\\n",
    "            u'\\uff0d', u'\\u00b4'\n",
    "        ]\n",
    "        # the resulting data structure which has the tokens as keys\n",
    "        # and the Index objects as values\n",
    "        self.indices = {}\n",
    "        # regex to match urls (taken from the web)\n",
    "        self.urlregex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]'\n",
    "                                   '|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        # keep @ to be able to recognize usernames\n",
    "        self.punctuation = string.punctuation.replace('@', '') + \\\n",
    "                           ''.join(self.unicodes2remove)\n",
    "        self.punctuation = self.punctuation.replace('#', '')\n",
    "        self.punctuation = self.punctuation.replace('…', '')\n",
    "        # a bunch of emoji unicodes\n",
    "        self.emojis = ''.join(emoji.UNICODE_EMOJI)\n",
    "        self.emojis = self.emojis.replace('#', '')\n",
    "        # combined english and german stop words\n",
    "        self.stop_words = set(stopwords.words('english') + stopwords.words('german'))\n",
    "        self.engSpellCheck = self._initSpellCheck('english')\n",
    "        self.gerSpellCheck = self._initSpellCheck('german')\n",
    "        self.correctedTerms = []    # For demonstration purposes only\n",
    "\n",
    "    def clean(self, s):\n",
    "        \"\"\"\n",
    "        Normalizes a string (tweet) by removing the urls, punctuation, digits,\n",
    "        emojis, by putting everything to lowercase and removing the\n",
    "        stop words. Tokenization is performed aswell.\n",
    "\n",
    "        :param s the string (tweet) to clean\n",
    "        :return: returns a list of cleaned tokens\n",
    "        \"\"\"\n",
    "        s = ' '.join(s.replace('[NEWLINE]', '').split())\n",
    "        s = ' '.join(s.replace('…', '...').split())\n",
    "        s = self.urlregex.sub('', s).strip()\n",
    "        s = s.translate(str.maketrans('', '', self.punctuation + string.digits \\\n",
    "                                      + self.emojis)).strip()\n",
    "        s = ' '.join(s.split())\n",
    "        s = s.lower()\n",
    "        s = self.tokenizer.tokenize(s)\n",
    "        s = [w for w in s if w not in self.stop_words]\n",
    "        return s\n",
    "\n",
    "    def _detectLanguage(self, context):\n",
    "        \"\"\"\n",
    "        Detects the language of a tweet based on a hierarchy of criteria:\n",
    "        1. the number of stopwords from each language in a tweet\n",
    "        2. the number of \"normal\" words from each language in a tweet\n",
    "        3. the more common language, in this case English\n",
    "        :param context: \n",
    "        :return: the determined language of the tweet\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(context)\n",
    "        stopsEN = [token for token in tokens if token in stopwords.words('english')]\n",
    "        stopsDE = [token for token in tokens if token in stopwords.words('german')]\n",
    "\n",
    "        # Chooses a language based on the number of stopwords\n",
    "        if len(stopsEN) > len(stopsDE):\n",
    "            return 'english'\n",
    "        elif len(stopsDE) > len(stopsEN):\n",
    "            return 'german'\n",
    "        # If that comparison isn't conclusive, it compares the number of words\n",
    "        # that exist in the respective dictionaries.\n",
    "        else:\n",
    "            cleaned = self.clean(context)\n",
    "\n",
    "            wordsEN = [token for token in cleaned if self.engSpellCheck.in_dictionary(token)]\n",
    "            wordsDE = [token for token in cleaned if self.gerSpellCheck.in_dictionary(token)]\n",
    "\n",
    "            if len(wordsEN) > len(wordsDE):\n",
    "                return 'english'\n",
    "            elif len(wordsDE) > len(wordsEN):\n",
    "                return 'german'\n",
    "            # If it still cannot decide, it defaults to the more common language: English\n",
    "            else:\n",
    "                return 'english'\n",
    "\n",
    "    @staticmethod\n",
    "    def _getGermanFreqDist():\n",
    "        \"\"\"\n",
    "        Walks through germanfreq.txt, splits the values into terms and frequencies\n",
    "        then maps them to a dictionary.\n",
    "\n",
    "        Ich\t489637 -> {ich: 489637}\n",
    "        ist\t475043 -> {ich: 489637, ist: 475043}\n",
    "        ich\t440346 -> {ich: 929983, ist: 475043} - adds to the count of capital 'ich'\n",
    "\n",
    "        :return: a frequency distribution dictionary of German words\n",
    "        \"\"\"\n",
    "        with open('germanfreq.txt', 'r') as f:\n",
    "            fdist = {}\n",
    "            lines = f.read().splitlines()\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "\n",
    "                # List comprehension is necessary because the file is sometimes organize \"term freq\" and sometimes\n",
    "                # \"freq term\" and it only works because there are no cardinal number terms included in the file.\n",
    "                term = [p for p in parts if not p.isdigit()][0]\n",
    "                freq = int([p for p in parts if p.isdigit()][0])\n",
    "\n",
    "                # The entries are split into capital and lowercase; here we combine the two.\n",
    "                try:\n",
    "                    fdist[term] = fdist[term] + freq\n",
    "                except KeyError:\n",
    "                    fdist[term] = freq\n",
    "\n",
    "            return fdist\n",
    "\n",
    "    def _getTokens2ids(self):\n",
    "        \"\"\"\n",
    "        Indexes all the tokens and maps them to a list of tweetIDs.\n",
    "        :return: a dictionary visualized as {token: [tweetID1, tweetID2, ...]}\n",
    "        \"\"\"\n",
    "        # For the sake of time and presenting functionality, we're limiting the number\n",
    "        # of tweets that we are indexing.\n",
    "        MAX_DOCS_TO_INDEX = 25\n",
    "        i = 0\n",
    "\n",
    "        tokens2id = {}\n",
    "\n",
    "        for id, doc in self.id2doc.items():\n",
    "            doc = self.clean(doc)\n",
    "            language = self._detectLanguage(' '.join(doc))\n",
    "\n",
    "            # This print statement is for demonstration purposes \n",
    "            print(language, doc)\n",
    "\n",
    "            for t in doc:\n",
    "                if language == 'english':\n",
    "                    # We are specifically excluding handles and hashtags\n",
    "                    # Nor do we want to spellcheck words that are in the dictionary\n",
    "                    if t[0] not in ['@', '#'] and not self.engSpellCheck.in_dictionary(t):\n",
    "                        original = t\n",
    "                        t = self.spellCheck(t, language)\n",
    "\n",
    "                        # Collects corrected words for demonstration purposes\n",
    "                        if original != t:\n",
    "                            self.correctedTerms.append((original, t))\n",
    "\n",
    "                elif language == 'german':\n",
    "                    if t[0] not in ['@', '#'] and not self.gerSpellCheck.in_dictionary(t):\n",
    "                        original = t\n",
    "                        t = self.spellCheck(t, language)\n",
    "\n",
    "                        if original != t:\n",
    "                            self.correctedTerms.append((original, t))\n",
    "\n",
    "                if t in tokens2id.keys():\n",
    "                    tokens2id[t].add(id)\n",
    "                else:\n",
    "                    # a set is used to avoid multiple entries of the same tweetID\n",
    "                    tokens2id[t] = {id}\n",
    "\n",
    "            # Break the loop after MAX_DOCS_TO_INDEX iterations\n",
    "            i += 1\n",
    "            if i >= MAX_DOCS_TO_INDEX:\n",
    "                break\n",
    "\n",
    "        return tokens2id\n",
    "\n",
    "    def index(self, path):\n",
    "        \"\"\"\n",
    "        1) call the method to read the file in\n",
    "        2) iterate over the original datastructure id2doc which keeps the mapping\n",
    "        of the tweet ids to the actual tweets and do:\n",
    "            2a) preprocessing of the tweets\n",
    "            2b) create a mapping from each token to its postings list (tokens2id)\n",
    "        3) iterate over the just created mapping of tokens to their respective \n",
    "        postings lists (tokens2id) and do:\n",
    "            3a) calculate the size of the postingslist\n",
    "            3b) sort the postings list numerically in ascending order\n",
    "            3c) create a linked list for the postings list\n",
    "            3d) create the Index object with the size of the postings list and\n",
    "            the pointer to the postings list - add to the resulting datastructure \n",
    "        :param path: the path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.initId2doc(path)\n",
    "        self._indexPostings(self._getTokens2ids())\n",
    "\n",
    "    def _indexPostings(self, tokens2id):\n",
    "        \"\"\"\n",
    "        Creates an `Index` object, which contains a pointer to to the beginning\n",
    "        of a postings list for every key/token in the `tokens2id` dictionary. It\n",
    "        stores this in the master inverted index `self.indices`.\n",
    "\n",
    "        :param tokens2id: \n",
    "        \"\"\"\n",
    "        for t, ids in tokens2id.items():\n",
    "            # size of the postings list which belongs to token t\n",
    "            size = len(ids)\n",
    "            # sort in ascending order\n",
    "            ids = sorted(ids)\n",
    "            # use the first (and smallest) tweetID to be the head node of the \n",
    "            # linked list\n",
    "            node = PostingNode(ids[0])\n",
    "            # keep reference to the head of the linked list since node variable\n",
    "            # is going to be overridden\n",
    "            pointer = node\n",
    "            for id in ids[1:]:\n",
    "                # create further list items\n",
    "                n = PostingNode(id)\n",
    "                # and append to the linked list\n",
    "                node.next = n\n",
    "                # step further\n",
    "                node = n\n",
    "            # create the index object with size of the postings list \n",
    "            # and a link to the postings list itself\n",
    "            i = Index(size, pointer)\n",
    "            self.indices[t] = i\n",
    "\n",
    "    def initId2doc(self, path):\n",
    "        \"\"\"\n",
    "        Reads the file in and fills the id2doc datastructure.\n",
    "        :param path: path to the tweets.csv file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8', newline='') as f:\n",
    "            r = csv.reader(f, delimiter='\\t')\n",
    "            for line in r:\n",
    "                self.id2doc[line[1]] = line[4]\n",
    "        f.close()\n",
    "\n",
    "    def _initSpellCheck(self, lang):\n",
    "        \"\"\"\n",
    "        Initializes two `SpellChecker` objects given the path to their dictionary files.\n",
    "\n",
    "        :param lang: the language of the spell checker \n",
    "        :return: a `SpellChecker` object based on a dictionary in that language\n",
    "        \"\"\"\n",
    "\n",
    "        if lang == 'english':\n",
    "            # `SpellChecker` will use the Brown FreqDist if none is provided\n",
    "            freq_dist = None\n",
    "        elif lang == 'german':\n",
    "            # For German, we need to create our own.\n",
    "            freq_dist = self._getGermanFreqDist()\n",
    "        else:\n",
    "            raise Exception(f'{lang} is not a supported language.')\n",
    "\n",
    "        return SpellChecker(SpellChecker.DEFAULT_DICTIONARIES[lang], fdist=freq_dist)\n",
    "\n",
    "    def intersect(self, pointer1, pointer2):\n",
    "        \"\"\"\n",
    "        Computes the intersection for two postings lists.\n",
    "        :param pointer1: first postings list\n",
    "        :param pointer2: second postings list\n",
    "        :return: returns the intersection \n",
    "        \"\"\"\n",
    "        # create temporary head node\n",
    "        node = PostingNode('tmp')\n",
    "        # keep reference to head node\n",
    "        rvalpointer = node\n",
    "        while pointer1 and pointer2:\n",
    "            val1 = pointer1.val\n",
    "            val2 = pointer2.val\n",
    "            # only append to the linked list if the values are equal\n",
    "            if val1 == val2:\n",
    "                n = PostingNode(val1)\n",
    "                node.next = n\n",
    "                node = n\n",
    "                pointer1 = pointer1.next\n",
    "                pointer2 = pointer2.next\n",
    "            # otherwise the postings list with the smaller value \n",
    "            # at the current index moves one forward\n",
    "            elif val1 > val2:\n",
    "                pointer2 = pointer2.next\n",
    "            elif val1 < val2:\n",
    "                pointer1 = pointer1.next\n",
    "        # return from the second element on since the first was the temporary one\n",
    "        return rvalpointer.next\n",
    "\n",
    "    def _query(self, term, lang):\n",
    "        \"\"\"\n",
    "        Internal method to query for one term.\n",
    "        :param: term the word which was queried for \n",
    "        :param: lang the language of the term for spellchecking\n",
    "        :return: returns the Index object of the corresponding query term\n",
    "        \"\"\"\n",
    "        if lang == 'english':\n",
    "            if not self.engSpellCheck.in_dictionary(term):\n",
    "                term = self.spellCheck(term, lang)\n",
    "        elif lang == 'german':\n",
    "            if not self.gerSpellCheck.in_dictionary(term):\n",
    "                term = self.spellCheck(term, lang)\n",
    "\n",
    "        try:\n",
    "            return self.indices[term]\n",
    "        except KeyError:\n",
    "            return Index(0, PostingNode(''))\n",
    "\n",
    "    def query(self, *arg):\n",
    "        \"\"\"\n",
    "        Query method which can take any number of terms as arguments.\n",
    "        It uses the internal _query method to get the postings lists for the single \n",
    "        terms. It calculates the intersection of all postings lists.\n",
    "        :param *arg term arguments\n",
    "        :return: returns a list of tweetIDs which all contain the query terms\n",
    "        \"\"\"\n",
    "        language = self._detectLanguage(' '.join([t for t in arg]))\n",
    "        print(language)  # For demonstration\n",
    "\n",
    "        # at this point it's a list of Index objects\n",
    "        pointers = [self._query(t, language) for t in arg if t not in self.stop_words]\n",
    "        # here the Index objects get sorted by the size of the \n",
    "        # postings list they point to\n",
    "        pointers = sorted(pointers, key=lambda i: i.size)\n",
    "        # here it becomes a list of pointers to the postings lists\n",
    "        pointers = [i.pointer2postingsList for i in pointers]\n",
    "        # first pointer\n",
    "        intersection = pointers[0]\n",
    "        # step through the pointers\n",
    "        for p in pointers[1:]:\n",
    "            # intersection between the new postings list and the so far\n",
    "            # computed intersection\n",
    "            intersection = self.intersect(intersection, p)\n",
    "            # if at any point the intersection is empty there is \n",
    "            # no need to continue\n",
    "            if not intersection:\n",
    "                return []\n",
    "        # convert the resulting intersection to a normal list\n",
    "        rval = []\n",
    "        pointer = intersection\n",
    "        while pointer:\n",
    "            rval.append(pointer.val)\n",
    "            pointer = pointer.next\n",
    "\n",
    "        return rval\n",
    "\n",
    "    def spellCheck(self, term, lang):\n",
    "        \"\"\"Runs the relevant spellchecker method.\"\"\"\n",
    "        return {'english': self.engSpellCheck,\n",
    "                'german': self.gerSpellCheck}[lang].spell_check(term)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The number of tokens in the inverted index.\"\"\"\n",
    "        return len(self.indices.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors' Note Before We Begin\n",
    "\n",
    "For Assignment 1 we were docked points beause our postings list was not sorted. We believe this to be a mistake. In the `_indexPostings` methods in the `TwitterIR` class, the `ids` list is sorted before the linked list is created. This occurs in the method's second statement, `sorted(ids)`. Note that in the version of this code that we submitted for Assignment 1, this statement occurs in the `index` method, but for this assignment we created a subroutine for that section of code. Thank you! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Index\n",
    "Because the algorithm takes a significant amount of time to run, we have decided to index only the first 25 tweets, which should suffice to demonstrate the `spellcheck` and `_detectLanguage` algorithms.\n",
    "\n",
    "In the cell below are the 25 tweets represented as lists of tokenized terms, and printed adjacent to them are the results of the language detection algorithm. We are relying — however haphazardly — on the assumption that tweets (and queries) are generally written in a single language, which we then use to spellcheck the content of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english ['@knakatani', '@chikonjugular', '@joofford', '@steveblogs', 'says', 'lifetime', 'risk', 'cervical', 'cancer', 'japan', 'means', 'hpv', 'endemic', 'japan', 'screening', 'working', 'well']\n",
      "english ['@fischerkurt', 'lady', 'whats', 'tumor', '#kippcharts']\n",
      "german ['@kingsofmetal', 'diagnoseverdacht', 'nunmal', 'schwer', 'gerade', 'hausarzt', 'blutbild', 'meist', 'sehen', 'gerade', 'hormone', 'überprüft', 'erklärbare', 'gewichtseinlagerungen', 'ja', 'wasser', 'fett', 'kind', 'tumor']\n",
      "german ['@germanletsplay', '@quentin', '@lopoopl', '@levanni', '@igeloe', '@annelle', 'glückwunsch']\n",
      "english ['interesting', 'pcr', 'rate', 'major', 'centers', 'authors', 'argue', 'treatment', 'compliance', 'major', 'centers', 'see', 'database', 'think', 'rather', 'due', 'earlier', 'detection', 'smaller', 'tumors', 'pcr', 'look', 'deeper', '#crcsm']\n",
      "english ['new', 'nanobots', 'kill', 'cancerous', 'tumors', 'cutting', 'blood', 'supply', '#digitaleconomy', 'february', 'pm']\n",
      "german ['rip', 'salem', 'aufgrund', 'tumors', 'eingeschläfert']\n",
      "english ['cancerfighting', 'nanorobots', 'programmed', 'seek', 'destroy', 'tumors', 'study', 'shows', 'first', 'applications', 'dna', 'origami', 'nanomedicine', 'nanoroboter', 'schrumpfen', 'tumore', '#medtech']\n",
      "german ['@riptear', 'tumorsdat', 'leg', 'straight', 'mccain']\n",
      "english ['quote', 'one', 'statement', 'cancers']\n",
      "english ['@joyannreid', '@pampylu', 'wrong', 'think', 'trump', 'probleme', 'mere', 'small', 'symptom', 'systematic', 'cancer', 'ask', 'antigunkids', 'go', 'look', 'failures', 'us', 'democracy']\n",
      "english ['erstmal', 'nen', 'anti', 'cancer', 'stick', 'lunge', 'reintherapieren']\n",
      "english ['#usa', '#upi', '#news', 'broadcast', '#emetnewspress', 'obesity', 'may', 'cause', 'sudden', 'cardiac', 'arrest', 'young', 'people', 'study', 'says', 'obesity', 'high', 'blood', 'pressure', 'may', 'play', 'much', 'greater', 'role', 'sudden', 'cardiac', 'arrest', 'among', 'young', 'people', 'previously', 'thought', 'ne']\n",
      "english ['leseempfehlung', 'extraordinary', 'correlation', 'obesity', 'social', 'inequality']\n",
      "english ['@fazefuzzface', 'welcome', 'obesity']\n",
      "english ['@isonnylucas', 'thats', 'exactly', 'point', 'whataboutism', 'dont', 'want', 'face', 'problem', 'point', 'worse', 'problem', 'bfollowing', 'logic', 'yes', 'opioid', 'crisis', 'bad', 'obesity', 'affects', 'way', 'children', 'many', 'deathslike', 'never', 'solve', 'problem']\n",
      "english ['obese', 'adult', 'free', 'online', 'mobile', 'sex']\n",
      "english ['obese', 'ebony', 'porn', 'carrie', 'fisher', 'naked', 'pictures']\n",
      "english ['@obesetobeast', 'sad', 'days', 'cant', 'get', 'days', 'stuff', 'like', 'reducing', 'damage', 'stuff', 'doesnt', 'move', 'way']\n",
      "english ['fat', 'obese', 'sex', 'pictures', 'big', 'booty', 'asians', 'fucking']\n",
      "english ['obese', 'porn', 'gallery', 'overcome', 'sex', 'addiction']\n",
      "german ['amateur', 'downblouse', 'videos', 'get', 'hiv', 'oral', 'sex']\n",
      "german ['syphilis', 'kommt', 'wiederja', 'preptherapie', 'hiv', 'übertragungsinfektionsrisiko', 'nahezu', 'null', 'senkenaber', 'syphilis', 'kleine', 'ficker', 'schon', 'längst', 'dasyphilis', 'tut', 'anfang', 'weh', 'ende', 'beißt', 'arsch']\n",
      "english ['u', 'get', 'hiv', 'oral', 'sex', 'gif', 'teens']\n",
      "german ['anal', 'sex', 'hiv', 'nudeteenpussyvideos']\n"
     ]
    }
   ],
   "source": [
    "twitterIR = TwitterIR()\n",
    "twitterIR.index('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hpv' -> 'he'.\n",
      "'nunmal' -> 'neunmal'.\n",
      "'pcr' -> 'per'.\n",
      "'pcr' -> 'per'.\n",
      "'pm' -> 'am'.\n",
      "'rip' -> 'trip'.\n",
      "'nanorobots' -> 'nanobots'.\n",
      "'applications' -> 'application'.\n",
      "'tumore' -> 'tumor'.\n",
      "'cancers' -> 'cancer'.\n",
      "'probleme' -> 'problem'.\n",
      "'failures' -> 'failure'.\n",
      "'erstmal' -> 'resteal'.\n",
      "'nen' -> 'new'.\n",
      "'whataboutism' -> 'whatabouts'.\n",
      "'bfollowing' -> 'following'.\n",
      "'opioid' -> 'apioid'.\n",
      "'affects' -> 'affect'.\n",
      "'children' -> 'chidden'.\n",
      "'deathslike' -> 'deathlike'.\n",
      "'online' -> 'unline'.\n",
      "'porn' -> 'born'.\n",
      "'pictures' -> 'picture'.\n",
      "'pictures' -> 'picture'.\n",
      "'asians' -> 'asian'.\n",
      "'fucking' -> 'sucking'.\n",
      "'porn' -> 'born'.\n",
      "'get' -> 'gut'.\n",
      "'wiederja' -> 'wieder'.\n",
      "'senkenaber' -> 'senkender'.\n",
      "'dasyphilis' -> 'syphilis'.\n",
      "'hiv' -> 'his'.\n"
     ]
    }
   ],
   "source": [
    "for original, corrected in twitterIR.correctedTerms:\n",
    "\tprint(f\"'{original}' -> '{corrected}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Index\n",
    "### German\n",
    "\n",
    "Below we attempt to query the German terms 'Blutbild' and 'schwer', but by a slip of the finger we accidentally query the non-word 'blutbilt'. The `query` method prints out what it determines to be the language of the query and then prints the relevant TweetIDs. We then fetch the tweet itself and print it for those of us who have not memorized every tweet and its corresponding ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german\n",
      "['965695626150326273']\n",
      "@Kings_of_Metal Ohne Diagnoseverdacht ist es nunmal schwer, gerade für einen Hausarzt. Am Blutbild kann man meist nicht viel sehen, gerade wenn man nicht auch die Hormone überprüft. Nicht erklärbare Gewichtseinlagerungen können ja alles sein, von Wasser, Fett, Kind bis hin zum Tumor.\n"
     ]
    }
   ],
   "source": [
    "query_result = twitterIR.query('blutbilt', 'schwer')\n",
    "print(query_result)\n",
    "print(twitterIR.id2doc[query_result[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English\n",
    "\n",
    "Let's give it an English query: say, all tweets containing the words 'major', 'centers', and 'authors'. But alas! We are bested by the unforgiving cryptology that is English orthography and we instead query 'major', 'senters', and 'authers'. Luckily our spellchecker is there to bail us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n",
      "['965672579133566980']\n",
      "Interesting. ⬆️ pCR rate at major centers. Authors argue with ⬆️ treatment compliance at major centers. We see the same in our database. I think it’s rather due to earlier detection, smaller tumors ➡️ more pCR. Will look deeper into this. #crcsm https://t.co/QfL5g2Z5u9\n"
     ]
    }
   ],
   "source": [
    "query_result = twitterIR.query('major', 'senters', 'authers')\n",
    "print(query_result)\n",
    "print(twitterIR.id2doc[query_result[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written Assignments\n",
    "The solution to Task 1 is pictured in a separate document below.\n",
    "\n",
    "## Task 1\n",
    "\n",
    "According to cosine similarity, which document di is most relevant to the given query q? Use the log term frequency weight (1 + log10(tf), if tf > 0) as the weight for terms, as discussed in the lecture. What are the values for each comparison? Explain your solution and provide similarity measures for all query document pairs.\n",
    "- q algorithm intersection\n",
    "- d1 intersection algorithm for two documents is efficient\n",
    "- d2 intersection algorithm\n",
    "- d3 algorithm\n",
    "\n",
    "#### Solution\n",
    "<img src=\"task1.jpg\">\n",
    "\n",
    "## Task 2\n",
    "Answer the following questions about distributed indexing:\n",
    "- What information does the task description contain that the master gives to a parser?\n",
    "- What information does the parser report back to the master upon completion of the task?\n",
    "- What information does the task description contain that the master gives to an inverter?\n",
    "- What information does the inverter report back to the master upon completion of the task?\n",
    "\n",
    "#### Solution\n",
    "- The master tells the parser the location of a split or portion of the corpus on which the parser will work.\n",
    "- The parser returns a collection of terms paired with their corresponding document identifier, e.g. (‘onomatopoeia’, 123456789), stored in intermediate segment files, which are partitioned based on an arbitrary quality intrinsic to the terms. For example, one segment file could store terms beginning with letters ‘a-f’, another ‘g-p’, and another ‘q-z’.\n",
    "- The master then assigns a machine to be an inverter, which entails walking through a specific class of segment file, say those containing terms beginning with ‘a-f’. The inverter’s job is to create an index in which each term and a list of the documents it appears in represent a key-value pair.\n",
    "- The inverter returns all the postings for its assigned term partition.\n",
    "\n",
    "## Task 3\n",
    "Explain logarithmic merging in your own words. Include the motivation for this method in your explanation and make clear what advantages this method has in contrast to one auxiliary index and only one index on hard disk.\n",
    "How could you use use a distributed compute cluster (for instance with map-reduce) in combination with logarithmic merging? How would you distribute the different merge steps? Which advantages would your solution have and which disadvantages can occur?\n",
    "#### Solution\n",
    "In logarithmic merge, a relatively small index is kept in memory so that common queries can be accomplished without reading from disk. When this auxiliary index becomes too large, it is written to an index on disk. This index has its own size limit, which when reached initiates the creation of a new disk index with a size limit twice as large as the previous one and so and so forth. This minimizes the amount of time spent making expensive merges and allows the indices to dynamically scale with additions to the corpus.\n",
    "\n",
    "One potential implementation for a distributed compute cluster would be assigning each compute node to be either a parser or inverter, with one lucky and reliable node being crowned the master. An additional set of compute nodes then interefaces with the partitions that the inverters create. Each of these nodes is responsible for holding a partition and carrying out their own logarithmic merge for it.\n",
    "\n",
    "## Task 4\n",
    "Heaps’ law is an empirical law.\n",
    "-- See PDF for collection properties. -- \n",
    "- K means kilo: times 1000\n",
    "- M means mega: times 1000000\n",
    "- G means giga: times 1000000000\n",
    "\n",
    "### Task 4.1\n",
    "Compute the coefficients k and b.\n",
    "#### Solution\n",
    "k ≈ .5\n",
    "b ≈ 30\n",
    "\n",
    "### Task 4.2\n",
    "Compute the expected vocabulary size for the complete collection (1G tokens).\n",
    "#### Solution\n",
    "M ≈ 1,000,000\n",
    "\n",
    "## Task 5\n",
    "Calculate the variable byte code and the gamma code for 217.\n",
    "#### Solution\n",
    "- bin(217) = 11011001\n",
    "- VB(217) = 00000001 11011001\n",
    "- 𝜸(217) = 111111101011001\n",
    "\n",
    "## Task 6\n",
    "From the following sequence of γ-coded gaps, reconstruct first the gap sequence and then the postings sequence:\n",
    "  `11110100001111101010111000`\n",
    "#### Solution\n",
    "- Gap sequence: [24, 1, 53, 4]\n",
    "- Posting sequence: [24, 25, 78, 82]\n",
    "\n",
    "## Task 7\n",
    "Describe in your own words: What is the advantage of the k-gram index vs. the permuterm index for handling wildcard queries?\n",
    "In general, would you take into account properties of the language for the decision which of the two approaches you prefer? Please explain!\n",
    "#### Solution\n",
    "The k-gram’s principle advantage is that is does not suffer from the permuterm index’s principle disadvantage, which is that a permuted index increases the size of the lexicon by several orders of magnitude. This phenomenon is exacerbated as the size of a lexicon increases. As such, lexicons that are larger in number or average word length are even further handicapped than smaller ones. The k-gram index does, however, have the disadvantage of having a very high recall but low precision because it returns more documents then are actually needed. The trade-off here is storage for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"task1.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
