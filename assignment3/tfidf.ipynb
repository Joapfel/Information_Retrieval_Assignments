{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some observations    \n",
    "===============\n",
    "\n",
    "**vocabulary size**    \n",
    "tweet tokenizer / no preprocessing = 313803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import sys\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv', sep='\\t', usecols=[1,4], names=['id', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = df['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "unicodes2remove = [\n",
    "    # all kinds of quotes\n",
    "    u'\\u2018', u'\\u2019', u'\\u201a', u'\\u201b', u'\\u201c', \\\n",
    "    u'\\u201d', u'\\u201e', u'\\u201f', u'\\u2014',\n",
    "    # all kinds of hyphens\n",
    "    u'\\u002d', u'\\u058a', u'\\u05be', u'\\u1400', u'\\u1806', \\\n",
    "    u'\\u2010', u'\\u2011', u'\\u2012', u'\\u2013',\n",
    "    u'\\u2014', u'\\u2015', u'\\u2e17', u'\\u2e1a', u'\\u2e3a', \\\n",
    "    u'\\u2e3b', u'\\u2e40', u'\\u301c', u'\\u3030',\n",
    "    u'\\u30a0', u'\\ufe31', u'\\ufe32', u'\\ufe58', u'\\ufe63', \\\n",
    "    u'\\uff0d', u'\\u00b4'\n",
    "]\n",
    "\n",
    "punctuation = string.punctuation.replace('@', '') + ''.join(unicodes2remove)\n",
    "# regex to match urls (taken from the web)\n",
    "urlregex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]'\n",
    "                           '|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "# keep @ to be able to recognize usernames\n",
    "punctuation = string.punctuation.replace('@', '') + ''.join(unicodes2remove)\n",
    "punctuation = punctuation.replace('#', '')\n",
    "# a bunch of emoji unicodes\n",
    "emojis = ''.join(emoji.UNICODE_EMOJI)\n",
    "emojis = emojis.replace('#', '')\n",
    "# combined english and german stop words\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Normalizes a string (tweet) by removing the urls, punctuation, digits,\n",
    "    emojis, by putting everything to lowercase and removing the\n",
    "    stop words. Tokenization is performed aswell.\n",
    "\n",
    "    :param s the string (tweet) to clean\n",
    "    :return: returns a list of cleaned tokens\n",
    "    \"\"\"\n",
    "    s = ' '.join(s.replace('[NEWLINE]', '').split())\n",
    "    s = ' '.join(s.replace('…', '...').split())\n",
    "    s = urlregex.sub('', s).strip()\n",
    "    s = s.translate(str.maketrans('', '', punctuation + string.digits \\\n",
    "                                  + emojis)).strip()\n",
    "    s = ' '.join(s.split())\n",
    "    s = s.lower()\n",
    "    s = tokenizer.tokenize(s)\n",
    "    s = [w for w in s if w not in stop_words]\n",
    "    return s if s else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@ Brandon: this takes a while use pickled version instead (below)\n",
    "#V = set()\n",
    "#for tweet in tweets:\n",
    "#    [V.add(token) for token in tokenizer.tokenize(tweet)]\n",
    "\n",
    "#TODO: we can basically reuse our old code here which cleans and tokenized the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['tweet'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set(chain.from_iterable(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-4354387c846e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-7675d810f8e5>\u001b[0m in \u001b[0;36mclean_gen\u001b[0;34m(vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-0ee3c36847ad>\u001b[0m in \u001b[0;36mclean\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     s = s.translate(str.maketrans('', '', punctuation + string.digits \\\n\u001b[0;32m---> 14\u001b[0;31m                                   + emojis)).strip()\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "cleaned = [clean(w) for w in w]\n",
    "time = datetime.now() - start\n",
    "print(cleaned)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313803"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump\n",
    "with open('vocabulary_naive.pickle', 'wb') as f:\n",
    "    pickle.dump(V, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "\"\"\"\n",
    "f = open('vocabulary_naive.pickle', 'rb')\n",
    "V = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['armselig',\n",
       " 'Feinmotorik',\n",
       " '#Abitur2018',\n",
       " 'realitätsverweigerung',\n",
       " 'factor-like',\n",
       " 'papu',\n",
       " '275',\n",
       " 'Sepsis',\n",
       " 'Maffiawaschmaschine',\n",
       " 'https://t.co/9JdHg2J519']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(V)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(doc1, doc2, V, tweets):\n",
    "    \"\"\"\n",
    "    d1,d2 -> list of tokens\n",
    "    \"\"\"\n",
    "    #i think we can use this intersection here because words that are in the in one \n",
    "    #vector but not in the other result in zero multiplication\n",
    "    intersect = set(doc1).intersection(set(doc2))\n",
    "    c1 = Counter(doc1)#token count\n",
    "    c2 = Counter(doc2)#token count\n",
    "    d1 = {}\n",
    "    d2 = {}\n",
    "    for t in doc1:\n",
    "        if t not in d1.keys() and t in intersect:\n",
    "            #tf idf\n",
    "            tf = c1[t]\n",
    "            df = len([tweet for tweet in tweets if t in tweet]) #naive string matching\n",
    "            idf = len(V) / df\n",
    "            tfidf = (1 + math.log10(tf)) * (math.log10(idf))\n",
    "            d1[t] = tfidf\n",
    "    for t in doc2:\n",
    "        if t not in d2.keys() and t in intersect:\n",
    "            #tf idf\n",
    "            tf = c2[t]\n",
    "            \n",
    "            #naive string matching\n",
    "            #TODO: after tokenization and preprocessing was improved \n",
    "            #we should look if the token is in a tweet by comparing single tokens and not \n",
    "            #the whole string\n",
    "            df = len([tweet for tweet in tweets if t in tweet])\n",
    "            idf = len(V) / df\n",
    "            tfidf = (1 + math.log10(tf)) * (math.log10(idf))\n",
    "            d2[t] = tfidf\n",
    "            df_tfidf = pd.DataFrame().from_dict(d1, orient='index')\n",
    "            df_tfidf[1] = pd.DataFrame().from_dict(d2, orient='index')\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Brandon: its fun to play around with the those documents\n",
    "doc1 = \"this is a random tweet Hausarzt Affe Affe Affe\".split()\n",
    "doc2 = \"this is random a a a a a tweet Hausarzt Hausarzt Hausarzt Hausarzt Hausarzt I think bla foo Affe\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-000433b72ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_tfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "df_tfidf = tfidf(doc1, doc2, V, tweets)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>965734992633565184</td>\n",
       "      <td>@knakatani @ChikonJugular @joofford @SteveBlog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>965706998946893824</td>\n",
       "      <td>@FischerKurt Lady, what´s a tumor? #KippCharts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>965695626150326273</td>\n",
       "      <td>@Kings_of_Metal Ohne Diagnoseverdacht ist es n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>965692298343444481</td>\n",
       "      <td>@GermanLetsPlay @Quentin34013799 @_Lopoopl_ @L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>965672579133566980</td>\n",
       "      <td>Interesting. ⬆️ pCR rate at major centers. Aut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                              tweet\n",
       "0  965734992633565184  @knakatani @ChikonJugular @joofford @SteveBlog...\n",
       "1  965706998946893824     @FischerKurt Lady, what´s a tumor? #KippCharts\n",
       "2  965695626150326273  @Kings_of_Metal Ohne Diagnoseverdacht ist es n...\n",
       "3  965692298343444481  @GermanLetsPlay @Quentin34013799 @_Lopoopl_ @L...\n",
       "4  965672579133566980  Interesting. ⬆️ pCR rate at major centers. Aut..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(vec1, vec2):\n",
    "    nominator = 0\n",
    "    denominator = 0\n",
    "    vec1_length = 0\n",
    "    vec2_length = 0\n",
    "    for v1,v2 in zip(vec1,vec2):\n",
    "        nominator += v1*v2\n",
    "        vec1_length += v1*v1\n",
    "        vec2_length += v2*v2\n",
    "    vec1_length = math.sqrt(vec1_length)\n",
    "    vec2_length = math.sqrt(vec2_length)\n",
    "    denominator = vec1_length * vec2_length \n",
    "    return nominator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298679480866374"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(df_tfidf[0], df_tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
