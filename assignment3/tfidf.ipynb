{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some observations    \n",
    "===============\n",
    "\n",
    "**vocabulary size**    \n",
    "tweet tokenizer / no preprocessing = 313803    \n",
    "tweet tokenizer / with leaning method = 260580"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create the Vocab Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import sys\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv', sep='\\t', usecols=[1,4], names=['id', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweets = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "unicodes2remove = [\n",
    "    # all kinds of quotes\n",
    "    u'\\u2018', u'\\u2019', u'\\u201a', u'\\u201b', u'\\u201c', \\\n",
    "    u'\\u201d', u'\\u201e', u'\\u201f', u'\\u2014',\n",
    "    # all kinds of hyphens\n",
    "    u'\\u002d', u'\\u058a', u'\\u05be', u'\\u1400', u'\\u1806', \\\n",
    "    u'\\u2010', u'\\u2011', u'\\u2012', u'\\u2013',\n",
    "    u'\\u2014', u'\\u2015', u'\\u2e17', u'\\u2e1a', u'\\u2e3a', \\\n",
    "    u'\\u2e3b', u'\\u2e40', u'\\u301c', u'\\u3030',\n",
    "    u'\\u30a0', u'\\ufe31', u'\\ufe32', u'\\ufe58', u'\\ufe63', \\\n",
    "    u'\\uff0d', u'\\u00b4'\n",
    "]\n",
    "\n",
    "punctuation = string.punctuation.replace('@', '') + ''.join(unicodes2remove)\n",
    "# regex to match urls (taken from the web)\n",
    "urlregex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]'\n",
    "                           '|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "# keep @ to be able to recognize usernames\n",
    "punctuation = string.punctuation.replace('@', '') + ''.join(unicodes2remove)\n",
    "punctuation = punctuation.replace('#', '')\n",
    "# a bunch of emoji unicodes\n",
    "emojis = ''.join(emoji.UNICODE_EMOJI)\n",
    "emojis = emojis.replace('#', '')\n",
    "# combined english and german stop words\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Normalizes a string (tweet) by removing the urls, punctuation, digits,\n",
    "    emojis, by putting everything to lowercase and removing the\n",
    "    stop words. Tokenization is performed aswell.\n",
    "\n",
    "    :param s the string (tweet) to clean\n",
    "    :return: returns a list of cleaned tokens\n",
    "    \"\"\"\n",
    "    s = ' '.join(s.replace('[NEWLINE]', '').split())\n",
    "    s = ' '.join(s.replace('â€¦', '...').split())\n",
    "    s = urlregex.sub('', s).strip()\n",
    "    s = s.translate(str.maketrans('', '', punctuation + string.digits \\\n",
    "                                  + emojis)).strip()\n",
    "    s = ' '.join(s.split())\n",
    "    s = s.lower()\n",
    "    s = tokenizer.tokenize(s)\n",
    "    s = [w for w in s if w not in stop_words]\n",
    "    return s if s else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenized = df['tweet'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "V = set(chain.from_iterable(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cleaned = [clean(w) for w in V]\n",
    "V = [word[0] for word in cleaned if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260580"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump\n",
    "with open('vocabulary_naive.pickle', 'wb') as f:\n",
    "    pickle.dump(V, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "\n",
    "f = open('vocabulary_naive.pickle', 'rb')\n",
    "V = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tfidf(doc1, doc2, V, tweets):\n",
    "    \"\"\"\n",
    "    d1,d2 -> list of tokens\n",
    "    \"\"\"\n",
    "    #i think we can use this intersection here because words that are in the in one \n",
    "    #vector but not in the other result in zero multiplication\n",
    "    intersect = set(doc1).intersection(set(doc2))\n",
    "    if not intersect:\n",
    "        return pd.DataFrame({})\n",
    "        \n",
    "    c1 = Counter(doc1)#token count\n",
    "    c2 = Counter(doc2)#token count\n",
    "    d1 = {}\n",
    "    d2 = {}\n",
    "    for t in doc1:\n",
    "        if t not in d1.keys() and t in intersect:\n",
    "            #tf idf\n",
    "            tf = c1[t]\n",
    "            df = len([tweet for tweet in tweets if t in tweet]) #naive string matching\n",
    "            if df == 0:\n",
    "                continue\n",
    "            else:\n",
    "                idf = len(V) / df\n",
    "                tfidf = (1 + math.log10(tf)) * (math.log10(idf))\n",
    "            d1[t] = tfidf\n",
    "    for t in doc2:\n",
    "        if t not in d2.keys() and t in intersect:\n",
    "            #tf idf\n",
    "            tf = c2[t]\n",
    "            #naive string matching\n",
    "            #TODO: after tokenization and preprocessing was improved \n",
    "            #we should look if the token is in a tweet by comparing single tokens and not \n",
    "            #the whole string\n",
    "            df = len([tweet for tweet in tweets if t in tweet])\n",
    "            if df == 0:\n",
    "                continue\n",
    "            else:\n",
    "                idf = len(V) / df\n",
    "                tfidf = (1 + math.log10(tf)) * (math.log10(idf))\n",
    "            d2[t] = tfidf\n",
    "            \n",
    "    df_tfidf = pd.DataFrame().from_dict(d1, orient='index')\n",
    "    df_tfidf[1] = pd.DataFrame().from_dict(d2, orient='index')\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(vec1, vec2):\n",
    "    if len(vec1) == 0 or len(vec2) == 0:\n",
    "        return 0\n",
    "    nominator = 0\n",
    "    denominator = 0\n",
    "    vec1_length = 0\n",
    "    vec2_length = 0\n",
    "    for v1,v2 in zip(vec1,vec2):\n",
    "        nominator += v1*v2\n",
    "        vec1_length += v1*v1\n",
    "        vec2_length += v2*v2\n",
    "    vec1_length = math.sqrt(vec1_length)\n",
    "    vec2_length = math.sqrt(vec2_length)\n",
    "    denominator = vec1_length * vec2_length \n",
    "    return nominator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Execute the example docs you want to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Brandon: its fun to play around with the those documents\n",
    "doc1 = \"this is a random tweet Hausarzt Affe Affe Affe\".split()\n",
    "doc2 = \"this is random a a a a a tweet Hausarzt Hausarzt Hausarzt Hausarzt Hausarzt I think bla foo Affe\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"i don't think society understands how hurtful it is when this kind of behavior by the POTUS becomes an accepted form of political discourse\".split()\n",
    "doc2 = 'And it is grievously hurtful to our society when vilification becomes an accepted form of political debate and negative campaigning becomes a full-time occupation.'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"He was a sk8er boi, she said see you later boy\".split()\n",
    "doc2 = \"I'm with the sk8er boi, I said see you later boy\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = 'and she told me Ich sitze noch in der KÃ¼che'.split()\n",
    "doc2 = 'Was meinst du mit sitting here with nachos'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>society</th>\n",
       "      <td>3.336760</td>\n",
       "      <td>3.336760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hurtful</th>\n",
       "      <td>4.415941</td>\n",
       "      <td>4.415941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.672525</td>\n",
       "      <td>0.672525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.635602</td>\n",
       "      <td>0.635602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>2.097460</td>\n",
       "      <td>2.097460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1.433886</td>\n",
       "      <td>1.102116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>becomes</th>\n",
       "      <td>3.884462</td>\n",
       "      <td>5.053802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.546041</td>\n",
       "      <td>0.546041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accepted</th>\n",
       "      <td>3.871873</td>\n",
       "      <td>3.871873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>form</th>\n",
       "      <td>2.304343</td>\n",
       "      <td>2.304343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political</th>\n",
       "      <td>3.660066</td>\n",
       "      <td>3.660066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1\n",
       "society    3.336760  3.336760\n",
       "hurtful    4.415941  4.415941\n",
       "it         0.672525  0.672525\n",
       "is         0.635602  0.635602\n",
       "when       2.097460  2.097460\n",
       "of         1.433886  1.102116\n",
       "becomes    3.884462  5.053802\n",
       "an         0.546041  0.546041\n",
       "accepted   3.871873  3.871873\n",
       "form       2.304343  2.304343\n",
       "political  3.660066  3.660066"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = tfidf(doc1, doc2, V, tweets)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933168703918781"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(df_tfidf[0], df_tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
