{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "from math import log10\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from iwnlp.iwnlp_wrapper import IWNLPWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Pandas library to work with the data. In the cell below, we populate `COLUMN_LABELS` with the relevant columns and then import the training and test data sets. Then we print the first five elements to visualize the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_LABELS = ['Game Name', 'Class', 'Title', 'Review Text']\n",
    "train = pd.read_csv('games-train.csv', sep='\\t', names=COLUMN_LABELS)\n",
    "test = pd.read_csv('games-test.csv', sep='\\t', names=COLUMN_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hay Day</td>\n",
       "      <td>gut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spaß pur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bike Race Free</td>\n",
       "      <td>gut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Top game mit sucht Potenzial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subway Surfers</td>\n",
       "      <td>gut</td>\n",
       "      <td>Gut</td>\n",
       "      <td>Es lagt manchmal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subway Surfers</td>\n",
       "      <td>gut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Es ist ein tolles Spiel aber manchmal bleibt e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hay Day</td>\n",
       "      <td>gut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cccccccccooooooooooooooooo ooooooooooll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Game Name Class Title  \\\n",
       "0         Hay Day   gut   NaN   \n",
       "1  Bike Race Free   gut   NaN   \n",
       "2  Subway Surfers   gut   Gut   \n",
       "3  Subway Surfers   gut   NaN   \n",
       "4         Hay Day   gut   NaN   \n",
       "\n",
       "                                         Review Text  \n",
       "0                                           Spaß pur  \n",
       "1                       Top game mit sucht Potenzial  \n",
       "2                                   Es lagt manchmal  \n",
       "3  Es ist ein tolles Spiel aber manchmal bleibt e...  \n",
       "4            Cccccccccooooooooooooooooo ooooooooooll  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Farmville 2</td>\n",
       "      <td>schlecht</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Echt schlecht , immer wen ich versuche zu star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Die Simpsons</td>\n",
       "      <td>gut</td>\n",
       "      <td>Buchi0202136</td>\n",
       "      <td>Suche noch freunde zum hinzufuegen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Simpsons</td>\n",
       "      <td>gut</td>\n",
       "      <td>Suchtgefähr :) !!</td>\n",
       "      <td>Ich find das Spiel gut,man muss nicht permanen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Die Simpsons</td>\n",
       "      <td>gut</td>\n",
       "      <td>Dauerhafter Spaß...</td>\n",
       "      <td>... durch immer neue Events. Schon 1 1/2 Jahre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subway Surfers</td>\n",
       "      <td>gut</td>\n",
       "      <td>Great</td>\n",
       "      <td>I like the game but near the last update it st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Game Name     Class                Title  \\\n",
       "0     Farmville 2  schlecht                  NaN   \n",
       "1    Die Simpsons       gut         Buchi0202136   \n",
       "2    Die Simpsons       gut    Suchtgefähr :) !!   \n",
       "3    Die Simpsons       gut  Dauerhafter Spaß...   \n",
       "4  Subway Surfers       gut                Great   \n",
       "\n",
       "                                         Review Text  \n",
       "0  Echt schlecht , immer wen ich versuche zu star...  \n",
       "1                 Suche noch freunde zum hinzufuegen  \n",
       "2  Ich find das Spiel gut,man muss nicht permanen...  \n",
       "3  ... durch immer neue Events. Schon 1 1/2 Jahre...  \n",
       "4  I like the game but near the last update it st...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We use the SpaCy library for tokenization and a SpaCy extension class for German lemmatization. We then define a wrapper around the `lemmatizer`'s `lemmatize` function to fit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = IWNLPWrapper(lemmatizer_path='IWNLP.Lemmatizer_20181001.json')\n",
    "nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(token):\n",
    "    \"\"\"\n",
    "    This function is a wrapper for the above lemmatizer. It modifies two\n",
    "    behaviors:\n",
    "    \n",
    "        - when the lemmatizer cannot confidently predict a lemma, it returns\n",
    "          None; this method returns the original token.\n",
    "        - when the lemmatizer finds more than one possible lemma, it returns\n",
    "          a list of the potential lemmmas; this method always chooses the first\n",
    "          option.\n",
    "          \n",
    "    Args:\n",
    "        token: a spacy.Token object representing a single token\n",
    "        \n",
    "    Returns:\n",
    "        the first element in the lemma list or else the original token\n",
    "    \"\"\"\n",
    "    lem = lemmatizer.lemmatize(str(token), pos_universal_google=token.pos_)\n",
    "    if not lem:\n",
    "        return token \n",
    "    else:\n",
    "        return lem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    \"\"\"\n",
    "    Cleans a document by:\n",
    "    - ensuring that it is indeed a string\n",
    "    - converting it to lowercase\n",
    "    - removing punctuation\n",
    "    - removing German stopwords\n",
    "    \n",
    "    Args:\n",
    "        doc: a given string\n",
    "        \n",
    "    Returns:\n",
    "        an array containing cleaned and tokenized terms for the string\n",
    "    \"\"\"\n",
    "    doc = str(doc).lower() # str() in case Pandas imported number as int type\n",
    "    doc = doc.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "    return [lemmatize(token) for token in nlp(doc)\n",
    "            if token.text not in stopwords.words('german')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_parameters(docs, collection_size): # docs = docs belonging to one class\n",
    "    \"\"\"\n",
    "    Estimates the parameters of a given class.\n",
    "    \n",
    "    Args:\n",
    "        docs: a collection of lists of preprocessed terms\n",
    "        collection_size: the size of the overall collection\n",
    "        \n",
    "    Returns:\n",
    "        a tuple containing the following two variables:\n",
    "        \n",
    "            - p_y: the portion of the overall collection that these docs make up\n",
    "            - count: a frequency distribution of terms in the docs\n",
    "    \"\"\"\n",
    "    p_y = len(docs) / collection_size \n",
    "    count = Counter()\n",
    "    for doc in docs:\n",
    "        count.update(preprocess(doc))\n",
    "        \n",
    "    return (p_y, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's estimate the parameters for each class in the data sets. We use Python's dict comprehension to map every unique value in the training data's class column (in this case just 'gut', 'schlecht') to the result of running `estimate_parameters` on a data set containing only elements of that class. For added clarity, we enumerate the variables below.\n",
    "\n",
    "- `params` = a dictionary of class to frequency distribution of terms in class\n",
    "- `class_` = a string containing either \"gut\" or \"schlecht\"\n",
    "- `train` = the training data as a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    class_: estimate_parameters(\n",
    "        train[train['Class'] == class_]['Review Text'], # Gets only the text of the review\n",
    "        len(train)\n",
    "    ) for class_ in train['Class'].unique() # = ['gut', 'schlecht']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`p_y` for each class, or the distribution of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8230904656534169 0.1769095343465831\n"
     ]
    }
   ],
   "source": [
    "print(params['gut'][0], params['schlecht'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words in the \"_gut_\" frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machen', 14616),\n",
       " ('besser', 11035),\n",
       " ('geil', 10941),\n",
       " ('Spiel', 10281),\n",
       " ('spielen', 8567),\n",
       " ('gut', 6532),\n",
       " ('echt', 6217),\n",
       " ('cool', 5736),\n",
       " ('finden', 5179),\n",
       " ('Spaß', 4662)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['gut'][1].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 most common words in the \"_schlecht_\" frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kommen', 3741),\n",
       " ('Update', 3158),\n",
       " ('beheben', 3073),\n",
       " ('gehen', 2851),\n",
       " ('spielen', 2685),\n",
       " ('Spiel', 2399),\n",
       " ('neu', 2387),\n",
       " ('machen', 2116),\n",
       " ('Stern', 1902),\n",
       " ('bekommen', 1766)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['schlecht'][1].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model to Predict Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_doc, parameters):\n",
    "    \"\"\"Predicts the most probable class for a document.\"\"\"\n",
    "    test_doc = preprocess(test_doc)\n",
    "    probs = []\n",
    "    for class_, params in parameters.items():\n",
    "        tokens_prob = 0\n",
    "        p_y = params[0]\n",
    "        counter = params[1]\n",
    "        for token in test_doc:\n",
    "            token_rel_freq = counter[token] / sum(counter.values())\n",
    "            if token_rel_freq == 0:\n",
    "                continue\n",
    "            tokens_prob += log10(token_rel_freq)\n",
    "            \n",
    "        class_prob = log10(p_y) + tokens_prob\n",
    "        probs.append((class_, abs(class_prob)))\n",
    "            \n",
    "    return min(probs, key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by testing the `predict` method on some easy examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gut'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('tolles Spiel', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'schlecht'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('das Spiel stürtzt immer ab. bitte schnell beheben', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On both examples, it acted just as we would expect. Now let's move on to the test data set. Let's assign `result` to a `Series` equal to the the prediction of each row in the \"Review Text\" column of `test`, then print the first five results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test['Review Text'].apply(lambda x: predict(x, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    schlecht\n",
       "1         gut\n",
       "2    schlecht\n",
       "3         gut\n",
       "4         gut\n",
       "Name: Review Text, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize what this looks like, we'll create a `DataFrame` of the two sequences of predicted and true values, then print the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>schlecht</td>\n",
       "      <td>schlecht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gut</td>\n",
       "      <td>gut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gut</td>\n",
       "      <td>schlecht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gut</td>\n",
       "      <td>gut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gut</td>\n",
       "      <td>gut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       True Predicted\n",
       "0  schlecht  schlecht\n",
       "1       gut       gut\n",
       "2       gut  schlecht\n",
       "3       gut       gut\n",
       "4       gut       gut"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = pd.concat([test['Class'], pred], axis=1)\n",
    "joined.columns = ['True', 'Predicted']\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(target_class, true, predicted):\n",
    "    \"\"\"\n",
    "    Evaluates the precision, recall, and f-score of the model on a specific\n",
    "    class. It then returns a tuple containing those values as well as the\n",
    "    intermediate values of instances in which the model guessed true and was\n",
    "    correct, `tp`, instances in which it guessed true and was wrong, `fp`, and\n",
    "    instances in which it guessed false and was wrong.\n",
    "    \n",
    "    Args:\n",
    "        target_class: the target class\n",
    "        true: a list-like object containing the gold standard\n",
    "        predicted: a list-like object of the same shape containing the model's predictions\n",
    "        \n",
    "    Returns:\n",
    "        a tuple containing true positives, false positives, false negatives,\n",
    "        precision, recall, and f-score.\n",
    "    \"\"\"\n",
    "    if len(true) != len(predicted):\n",
    "        raise ValueError('Sequences are of different lengths.')\n",
    "    evl = pd.DataFrame(list(zip(true, predicted)), columns=['True', 'Predicted'])\n",
    "    tp = len(evl[(evl['Predicted'] == target_class) & (evl['True'] == target_class)])\n",
    "    fp = len(evl[(evl['Predicted'] == target_class) & (evl['True'] != target_class)])\n",
    "    fn = len(evl[(evl['Predicted'] != target_class) & (evl['True'] == target_class)])\n",
    "    prec = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    fscore = (2 * prec * recall) / (prec + recall)\n",
    "    return tp, fp, fn, prec, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gut: (33282, 3142, 3134, 0.913738194596969, 0.913938927943761, 0.913838550247117)\n",
      "schlecht: (4675, 3134, 3142, 0.5986682033551031, 0.5980555200204682, 0.5983617048508895)\n"
     ]
    }
   ],
   "source": [
    "print('gut:', evaluate('gut', test['Class'], pred))\n",
    "print('schlecht:', evaluate('schlecht', test['Class'], pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
